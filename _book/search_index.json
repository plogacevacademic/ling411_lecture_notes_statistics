[["index.html", "Statistics in R Chapter 1 Initial Remarks", " Statistics in R Pavel Logacev 2021-02-05 Chapter 1 Initial Remarks (05 February, 2021, 14:34) The purpose of these lecture notes is to allow you to revisit concepts covered in class. In the following, I will draw on the following resources. The pretty great introduction to R and statistics by Danielle Navarro available here. Matt Crump’s ‘Answering Questions with Data’. Bodo Winter’s ‘Statistics of Linguists: An Introduction Using R’ "],["scalesOfMeasurement.html", "Chapter 2 Scales of Measurement 2.1 Nominal scale 2.2 Ordinal scale 2.3 Interval scale 2.4 Ratio scale 2.5 Continuous versus discrete variables 2.6 Some complexities", " Chapter 2 Scales of Measurement (an adapted version of Danielle Navarro’s chapter 2.) (05 February, 2021, 14:34) As might recall, the outcome of a measurement is what we call a variable. But not all variables are of the same qualitative type, and it’s very useful to understand what types there are. A very useful concept for distinguishing between different types of variables is what’s known as scales of measurement. 2.1 Nominal scale A nominal scale variable (also referred to as a categorical variable) is one in which there is no particular relationship between the different possibilities. No ordering: For these kinds of variables it doesn’t make any sense to say that one of them is “bigger’ or “better” than any other one, and it doesn’t make any sense to average them. Examples: The classic example for this is “eye color”. Eyes can be blue, green and brown, among other possibilities, but none of them is any “better” than any other one. As a result, it would feel really odd to talk about an “average eye color”. Similarly, gender is nominal too: male isn’t better or worse than female, neither does it make sense to try to talk about an “average gender”. In short, nominal scale variables are those for which the only thing you can say about the different possibilities is that they are different. That’s it. Let’s take a slightly closer look at this. Suppose I was doing research on how people commute to and from work. One variable I would have to measure would be what kind of transportation people use to get to work. This “transport type” variable could have quite a few possible values, including: “train”, “bus”, “car”, “bicycle”, etc. For now, let’s suppose that these four are the only possibilities, and suppose that when I ask 100 people how they got to work today, and I get this: Transportation Number of people (1) Train 12 (2) Bus 30 (3) Car 48 (4) Bicycle 10 So, what’s the average transportation type? Obviously, the answer here is that there isn’t one. It’s a silly question to ask. You can say that travel by car is the most popular method, and travel by train is the least popular method, but that’s about all. 2.2 Ordinal scale Ordinal scale variables have a bit more structure than nominal scale variables, but not by a lot. An ordinal scale variable is one in which there is a natural, meaningful way to order the different possibilities, but you can’t do anything else. Examples: The usual example given of an ordinal variable is “finishing position in a race”. You can say that the person who finished first was faster than the person who finished second, but you don’t know how much faster. As a consequence we know that 1st &lt; 2nd, and we know that 2nd &lt; 3rd, but the difference between 1st and 2nd might be much larger than the difference between 2nd and 3rd. Here’s an more interesting example. Suppose I’m interested in people’s attitudes to climate change, and I ask them to pick one of these four statements that most closely matches their beliefs: Temperatures are rising, because of human activity Temperatures are rising, but we don’t know why Temperatures are rising, but not because of humans Temperatures are not rising These four statements actually do have a natural ordering, in terms of “the extent to which they agree with the current science”. Statement 1 is a close match, Statement 2 is a reasonable match, Statement 3 isn’t a very good match, Statement 4 is in strong opposition to the science. So, in terms of the thing I’m interested in (the extent to which people endorse the science), I can order the items as \\(1 &gt; 2 &gt; 3 &gt; 4\\). So, let’s suppose I asked 100 people these questions, and got the following answers: Response Number (1) Temperatures are rising, because of human activity 51 (2) Temperatures are rising, but we don’t know why 20 (3) Temperatures are rising, but not because of humans 10 (4) Temperatures are not rising 19 When analysing these data, it seems quite reasonable to try to group (1), (2) and (3) together, and say that 81 of 100 people were willing to at least partially endorse the science. And it’s also quite reasonable to group (2), (3) and (4) together and say that 49 of 100 people registered at least some disagreement with the dominant scientific view. However, it would be entirely bizarre to try to group (1), (2) and (4) together and say that 90 of 100 people said what? There’s nothing sensible that allows you to group those responses together at all. 2.3 Interval scale In contrast to nominal and ordinal scale variables, interval scale and ratio scale variables are variables for which the numerical value is genuinely meaningful. In the case of interval scale variables, the differences between the numbers are interpretable, but the variable doesn’t have a “natural” zero value. Examples: A good example of an interval scale variable is measuring temperature in degrees celsius. For instance, if it was 15° yesterday and 18° today, then the 3° difference between the two is genuinely meaningful. Moreover, that 3° difference is exactly the same as the 3° difference between 7° and 10°. In short, addition and subtraction are meaningful for interval scale variables. (Nevermind that a change of 10° will feel different at 20° compared to 30°.) However, notice that the 0° does not mean “no temperature at all”: it actually means “the temperature at which water freezes”, which is pretty arbitrary. As a consequence, it becomes pointless to try to multiply and divide temperatures. It is wrong to say that 20° is twice as hot as 10°, just as it is weird and meaningless to try to claim that 20° is negative two times as hot as 10°. Or suppose I’m interested in looking at how the attitudes of first-year university students have changed over time. Obviously, I’m going to want to record the year in which each student started. This is an interval scale variable. A student who started in 2003 did arrive 5 years before a student who started in 2008. However, it would be completely insane for me to divide 2008 by 2003 and say that the second student started “1.0024 times later” than the first one. That doesn’t make any sense at all. 2.4 Ratio scale The final type of variable to consider is a ratio scale variable, in which zero really means zero, and it’s okay to multiply and divide. Example: A good example of a ratio scale variable is response time (RT). In a lot of tasks it’s very common to record the amount of time somebody takes to solve a problem or answer a question, because it’s an indicator of how difficult the task is. Suppose that Alan takes 2.3 seconds to respond to a question, whereas Ben takes 3.1 seconds. As with an interval scale variable, addition and subtraction are both meaningful here. Ben really did take \\(3.1 - 2.3 = 0.8\\) seconds longer than Alan did. Notice that multiplication and division also make sense here too: Ben took \\(3.1/2.3 = 1.35\\) times as long as Alan did to answer the question. The reason why you can do this is that, for a ratio scale variable such as RT, “zero seconds” really does mean “no time at all”. Scale Meaningful order Meaningful differences Meaningful ratio Nominal scale Ordinal scale x Interval scale x x Ratio scale x x x 2.5 Continuous versus discrete variables There’s a second kind of distinction that you need to be aware of, regarding what types of variables you can run into. This is the distinction between continuous variables and discrete variables. The difference between these is as follows: A continuous variable is one in which, for any two values that you can think of, it’s always logically possible to have another value in between. A discrete variable is, in effect, a variable that isn’t continuous. For a discrete variable, it’s sometimes the case that there’s nothing in the middle. For exampl, response time is continuous. If Alan takes 3.1 seconds and Ben takes 2.3 seconds to respond to a question, then it’s possible for Cameron’s response time to lie in between, by taking 3.0 seconds. And of course it would also be possible for David to take 3.031 seconds to respond, meaning that his RT would lie in between Cameron’s and Alan’s. And while in practice it might be impossible to measure RT that precisely, it’s certainly possible in principle. Because we can always find a new value for RT in between any two other ones, we say that RT is continuous. Discrete variables occur when this rule is violated. For example, nominal scale variables are always discrete: there isn’t a type of transportation that falls “in between” trains and bicycles, not in the strict mathematical way that 2.3 falls in between 2 and 3. So transportation type is discrete. Similarly, ordinal scale variables are always discrete: although “2nd place” does fall between “1st place” and “3rd place”, there’s nothing that can logically fall in between “1st place” and “2nd place”. Interval scale and ratio scale variables can go either way. As we saw above, response time (a ratio scale variable) is continuous. Temperature in degrees celsius (an interval scale variable) is also continuous. However, the year you went to school (an interval scale variable) is discrete. There’s no year in between 2002 and 2003. The number of questions you get right on a true-or-false test (a ratio scale variable) is also discrete: since a true-or-false question doesn’t allow you to be “partially correct”, there’s nothing in between 5/10 and 6/10. 2.6 Some complexities Okay, I know you’re going to be shocked to hear this, but the real world is much messier than this little classification scheme suggests. Very few variables in real life actually fall into these nice neat categories, so you need to be kind of careful not to treat the scales of measurement as if they were hard and fast rules. It doesn’t work like that: they’re guidelines, intended to help you think about the situations in which you should treat different variables differently. Nothing more. So let’s take a classic example, maybe the classic example, of a psychological measurement tool: the Likert scale. The humble Likert scale is the bread and butter tool of all survey design. You yourself have filled out hundreds, maybe thousands of them, and odds are you’ve even used one yourself. Suppose we have a survey question that looks like this: Which of the following best describes your opinion of the statement that “all pirates are freaking awesome” and then the options presented to the participant are these: Strongly disagree Disagree Neither agree nor disagree Agree Strongly agree This set of items is an example of a 5-point Likert scale: people are asked to choose among one of several (in this case 5) clearly ordered possibilities, generally with a verbal descriptor given in each case. However, it’s not necessary that all items be explicitly described. This is a perfectly good example of a 5-point Likert scale too: Strongly disagree Strongly agree Likert scales are very handy, if somewhat limited, tools. The question is, what kind of variable are they? They’re obviously discrete, since you can’t give a response of 2.5. They’re obviously not nominal scale, since the items are ordered; and they’re not ratio scale either, since there’s no natural zero. But are they ordinal scale or interval scale? One argument says that we can’t really prove that the difference between “strongly agree” and “agree” is of the same size as the difference between “agree” and “neither agree nor disagree”. In fact, in everyday life it’s pretty obvious that they’re not the same at all. So this suggests that we ought to treat Likert scales as ordinal variables. On the other hand, in practice most participants do seem to take the whole “on a scale from 1 to 5” part fairly seriously, and they tend to act as if the differences between the five response options were fairly similar to one another. As a consequence, a lot of researchers treat Likert scale data as if it were interval scale. It’s not interval scale, but in practice it’s close enough that we usually think of it as being quasi-interval scale. "],["descriptives.html", "Chapter 3 Descriptive Statistics 3.1 Measures of central tendency 3.2 Measures of variability", " Chapter 3 Descriptive Statistics (an adapted version of Danielle Navarro’s chapter 3/5 on descriptive statistics.) (05 February, 2021, 14:34) Any time that you get a new data set to look at, one of the first tasks that you have to do is find ways of summarising the data in a compact, easily understood fashion. This is what descriptive statistics (as opposed to inferential statistics) is all about. In fact, to many people the term “statistics” is synonymous with descriptive statistics. Before going into any details, let’s take a moment to get a sense of why we need descriptive statistics. To do this, let’s load a dataset on sleep in mammals. mammalian_sleep &lt;- read.csv(&quot;./data/msleep_ggplot2.csv&quot;) %&gt;% dplyr::select(name, sleep_total, bodywt) %&gt;% dplyr::rename(sleep_total_h = sleep_total, bodywt_kg = bodywt) %&gt;% dplyr::mutate(sleep_total_h = round(sleep_total_h) ) head(mammalian_sleep) ## name sleep_total_h bodywt_kg ## 1 Cheetah 12 50.000 ## 2 Owl monkey 17 0.480 ## 3 Mountain beaver 14 1.350 ## 4 Greater short-tailed shrew 15 0.019 ## 5 Cow 4 600.000 ## 6 Three-toed sloth 14 3.850 There are three variables here, name, sleep_total_h and bodywt_kg. For each animal named in name, the sleep_total_h variable contains the average number of hours animals of this kind sleep per day. The variable bodywt_kg contains the average weight of that animal in kg. Let’s have a look at the sleep_total_h variable: print(mammalian_sleep$sleep_total_h) ## [1] 12 17 14 15 4 14 9 7 10 3 5 9 10 12 10 8 9 17 5 18 4 20 3 3 10 ## [26] 11 15 12 10 2 3 6 6 8 10 3 19 10 14 14 13 12 20 15 11 8 14 8 4 10 ## [51] 16 10 14 9 10 11 12 14 4 6 11 18 5 13 9 10 8 11 11 17 14 16 13 9 9 ## [76] 16 4 16 9 5 6 12 10 This output doesn’t make it easy to get a sense of what the data are actually saying. Just “looking at the data” isn’t a terribly effective way of understanding data. In order to get some idea about what’s going on, we need to calculate some descriptive statistics and draw some nice pictures. Let’s take a look at a histogram of these data: A histogram is a graphical representation of the frequencies of different values of ranges of values in the data. mammalian_sleep %&gt;% ggplot(aes(sleep_total_h)) + geom_histogram(binwidth = 1, color = &quot;black&quot;, fill = &quot;lightgrey&quot;) Figure 3.1: A histogram of the average amount of sleep by animal (the sleep_total_h variable). As you might expect, the larger the margin the less frequently you tend to see it. 3.1 Measures of central tendency Drawing pictures of the data, as I did in Figure 3.1 is an excellent way to convey the “gist” of what the data is trying to tell you, it’s often extremely useful to try to condense the data into a few simple “summary” statistics. In most situations, the first thing that you’ll want to calculate is a measure of central tendency. That is, you’d like to know something about the “average” or “middle” of your data lies. The two most commonly used measures are the mean, median and mode; occasionally people will also report a trimmed mean. I’ll explain each of these in turn, and then discuss when each of them is useful. 3.1.1 The mean The mean of a set of observations is just a normal, old-fashioned average: add all of the values up, and then divide by the total number of values. The first five animals’ typical amount of sleep is 12 + 17 + 14 + 15 + 4, so the mean of these observations is just: \\[ \\frac{12 + 17 + 14 + 15 + 4}{5} = \\frac{62.4}{5} = 12.48 \\] Of course, this definition of the mean isn’t news to anyone: averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I’ll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in R. The first piece of notation to introduce is \\(N\\), which we’ll use to refer to the number of observations that we’re averaging (in this case \\(N = 5\\)). Next, we need to attach a label to the observations themselves. It’s traditional to use \\(X\\) for this, and to use subscripts to indicate which observation we’re actually talking about. That is, we’ll use \\(X_1\\) to refer to the first observation, \\(X_2\\) to refer to the second observation, and so on, all the way up to \\(X_N\\) for the last one. Or, to say the same thing in a slightly more abstract way, we use \\(X_i\\) to refer to the \\(i\\)-th observation. Just to make sure we’re clear on the notation, the following table lists the 5 observations in the sleep_total_h variable, along with the mathematical symbol used to refer to it, and the actual value that the observation corresponds to: the observation its symbol the observed value Cheetah (animal 1) \\(X_1\\) 12 hours Owl monkey (animal 2) \\(X_2\\) 17 hours Mountain beaver (animal 3) \\(X_3\\) 14 hours Greater short-tailed shrew (animal 4) \\(X_4\\) 15 hours Cow (animal 5) \\(X_5\\) 4 hours Okay, now let’s try to write a formula for the mean. By tradition, we use \\(\\bar{X}\\) as the notation for the mean. So the calculation for the mean could be expressed using the following formula: \\[ \\bar{X} = \\frac{X_1 + X_2 + ... + X_{N-1} + X_N}{N} \\] This formula is entirely correct, but it’s terribly long, so we make use of the summation symbol \\(\\scriptstyle\\sum\\) to shorten it.1 If I want to add up the first five observations, I could write out the sum the long way, \\(X_1 + X_2 + X_3 + X_4 +X_5\\) or I could use the summation symbol to shorten it to this: \\[ \\sum_{i=1}^5 X_i \\] Taken literally, this could be read as “the sum, taken over all \\(i\\) values from 1 to 5, of the value \\(X_i\\)”. But basically, what it means is “add up the first five observations”. In any case, we can use this notation to write out the formula for the mean, which looks like this: \\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i \\] In all honesty, I can’t imagine that all this mathematical notation helps clarify the concept of the mean at all. In fact, it’s really just a fancy way of writing out the same thing I said in words: add all the values up, and then divide by the total number of items. However, that’s not really the reason I went into all that detail. My goal was to try to make sure that everyone reading this book is clear on the notation that we’ll be using throughout the book: \\(\\bar{X}\\) for the mean, \\(\\scriptstyle\\sum\\) for the idea of summation, \\(X_i\\) for the \\(i\\)th observation, and \\(N\\) for the total number of observations. We’re going to be re-using these symbols a fair bit, so it’s important that you understand them well enough to be able to “read” the equations, and to be able to see that it’s just saying “add up lots of things and then divide by another thing”. 3.1.2 Calculating the mean in R Okay that’s the maths, how do we get the magic computing box to do the work for us? If you really wanted to, you could do this calculation directly in R. For the first numbers, do this just by typing it in as if R were a calculator… (12 + 17 + 14 + 15 + 4) / 5 ## [1] 12.4 … in which case R outputs the answer 36.6, just as if it were a calculator. However, we learned quicker ways of doing that sum( mammalian_sleep$sleep_total_h[1:5] )/ 5 ## [1] 12.4 # or: mean( mammalian_sleep$sleep_total_h[1:5] ) ## [1] 12.4 3.1.3 The median The second measure of central tendency that people use a lot is the median, and it’s even easier to describe than the mean. The median of a set of observations is just the middle value. As before let’s imagine we were interested only in the first 5 animals: They sleep 12, 17, 14, 15, and 4 hours respectively. To figure out the median, we sort these numbers into ascending order: \\[ 4, 12, \\color{red}{14}, 15, 17 \\] From inspection, it’s obvious that the median value of these 5 observations is 32, since that’s the middle one in the sorted list (I’ve put it in red to make it even more obvious). Easy stuff. But what should we do if we were interested in the first 6 animals rather than the first 5? Since the sixth animal sleeps for 14 hours, our sorted list is now: \\[ 4, 12, \\color{red}{14}, \\color{red}{14}, 15, 17 \\] That’s also easy. It’s still 14. But what we do if we were interested in the first 8 animals? Here is our new sorted list. \\[ 4, 7, 9, \\color{red}{12}, \\color{red}{14}, 14, 15, 17 \\] There are now two middle numbers, 12 and 14. The median is defined as the average of those two numbers, which is of course 13. To understand why, think of the median as the value that divides the sorted list of numbers into two halves – those on its left, and those on its right. As before, it’s very tedious to do this by hand when you’ve got lots of numbers. To illustrate this, here’s what happens when you use R to sort all the sleep durations. First, I’ll use the sort() function to display the 83 numbers in increasing numerical order: sort( mammalian_sleep$sleep_total_h ) ## [1] 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 6 6 6 6 7 8 8 8 8 8 ## [26] 9 9 9 9 9 9 9 9 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 ## [51] 11 12 12 12 12 12 12 13 13 13 14 14 14 14 14 14 14 14 15 15 15 16 16 16 16 ## [76] 17 17 17 18 18 19 20 20 Because the vector is 83 elements long, the middle value is at position 42. This means that the median of this vector is 10. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life, we use the median command: median( mammalian_sleep$sleep_total_h ) ## [1] 10 which outputs the median value of 10. 3.1.4 Mean or median? What’s the difference? Figure 3.2: An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the “centre of gravity” of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger. Knowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. This is illustrated in Figure 3.2 the mean is kind of like the “centre of gravity” of the data set, whereas the median is the “middle value” in the data. What this implies, as far as which one you should use, depends a little on what type of data you’ve got and what you’re trying to achieve. As a rough guide: One consequence is that there’s systematic differences between the mean and the median when the histogram is asymmetric (skewed; see Section ??). This is illustrated in Figure 3.2 notice that the median (right hand side) is located closer to the “body” of the histogram, whereas the mean (left hand side) gets dragged towards the “tail” (where the extreme values are). To give a concrete example, suppose Bob (income $50,000), Kate (income $60,000) and Jane (income $65,000) are sitting at a table: the average income at the table is $58,333 and the median income is $60,000. Then Bill sits down with them (income $100,000,000). The average income has now jumped to $25,043,750 but the median rises only to $62,500. If you’re interested in looking at the overall income at the table, the mean might be the right answer; but if you’re interested in what counts as a typical income at the table, the median would be a better choice here. 3.1.5 Trimmed mean One of the fundamental rules of applied statistics is that the data are messy. Real life is never simple, and so the data sets that you obtain are never as straightforward as the statistical theory says.2 This can have awkward consequences. To illustrate, consider this rather strange looking data set (nevermind what it represents): \\[ -100,2,3,4,5,6,7,8,9,10 \\] If you were to observe this in a real life data set, you’d probably suspect that something funny was going on with the \\(-100\\) value. It’s probably an outlier, a value that doesn’t really belong with the others. You might consider removing it from the data set entirely, and in this particular case I’d probably agree with that course of action. In real life, however, you don’t always get such cut-and-dried examples. For instance, you might get this instead: \\[ -15,2,3,4,5,6,7,8,9,12 \\] The \\(-15\\) looks a bit suspicious, but not anywhere near as much as that \\(-100\\) did. In this case, it’s a little trickier. It might be a legitimate observation, it might not. When faced with a situation where some of the most extreme-valued observations might not be quite trustworthy, the mean is not necessarily a good measure of central tendency. It is highly sensitive to one or two extreme values, and is thus not considered to be a robust measure. One remedy that we’ve seen is to use the median. A more general solution is to use a “trimmed mean”. To calculate a trimmed mean, what you do is “discard” the most extreme examples on both ends (i.e., the largest and the smallest), and then take the mean of everything else. The goal is to preserve the best characteristics of the mean and the median: just like a median, you aren’t highly influenced by extreme outliers, but … like the mean, you “use” more than one of the observations. Generally, we describe a trimmed mean in terms of the percentage of observation on either side that are discarded. So, for instance, a 10% trimmed mean discards the largest 10% of the observations and the smallest 10% of the observations, and then takes the mean of the remaining 80% of the observations. Not surprisingly, the 0% trimmed mean is just the regular mean, and the 50% trimmed mean is the median. In that sense, trimmed means provide a whole family of central tendency measures that span the range from the mean to the median. For our toy example above, we have 10 observations, and so a 10% trimmed mean is calculated by ignoring the largest value (i.e., 12) and the smallest value (i.e., -15) and taking the mean of the remaining values. First, let’s enter the data dataset &lt;- c( -15,2,3,4,5,6,7,8,9,12 ) Next, let’s calculate means and medians: mean( dataset ) ## [1] 4.1 median( dataset ) ## [1] 5.5 That’s a fairly substantial difference, but I’m tempted to think that the mean is being influenced a bit too much by the extreme values at either end of the data set, especially the \\(-15\\) one. So let’s just try trimming the mean a bit. If I take a 10% trimmed mean, we’ll drop the extreme values on either side, and take the mean of the rest: mean( dataset, trim = .1) ## [1] 5.5 In this case it gives exactly the same answer as the median. Note that, to get a 10% trimmed mean you write trim = .1, not trim = 10. 3.1.6 Mode The mode is the last measure of central tendency we’ll look at. It is very simple: it is the value that occurs most frequently. Let’s look at the some soccer data: specifically, the European Cup and Champions League results in the time from 1955-2016. Lets find out which team has won the most matches. The command below tells R we just want the first 25 rows of the data.frame. library(engsoccerdata) table(champs$tiewinner) %&gt;% sort(decreasing=T) %&gt;% .[1:25] ## ## Real Madrid Bayern Munich SL Benfica AC Milan ## 183 134 110 103 ## Barcelona Liverpool Juventus Dinamo Kiev ## 101 97 87 82 ## Celtic Manchester United RSC Anderlecht AFC Ajax ## 81 81 75 71 ## Internazionale Steaua Bucuresti Crvena Zvezda Rangers ## 66 63 61 58 ## Partizan Belgrade PSV Eindhoven Dinamo Zagreb FC Porto ## 52 49 48 46 ## Atletico Madrid Panathinaikos BATE Borisov CSKA Sofia ## 45 44 40 40 ## Galatasaray ## 40 It appears that the mode of the winning team is ‘Real Madrid’. That doesn’t come as a surprise even to me. Of course, the mode is the right (and only) summary for nominal variables. But we can compute a mode for all types of variables. For example, let’s take a look at the mean, median, and mode of the total number of goals per game. champs %&lt;&gt;% mutate(total_goals = hgoal + vgoal) # total goals is home team goals + visitor goals mean(champs$total_goals) ## [1] 2.81642 median(champs$total_goals) ## [1] 3 modeOf(champs$total_goals) ## [1] 2 mean_goals &lt;- mean(champs$total_goals) median_goals &lt;- median(champs$total_goals) mode_goals &lt;- modeOf(champs$total_goals) ggplot(champs, aes(total_goals)) + geom_histogram() + geom_vline(xintercept = mean_goals, color = &quot;red&quot;) + geom_vline(xintercept = median_goals, color = &quot;blue&quot;) + geom_vline(xintercept = mode_goals, color = &quot;green&quot;) + geom_text(data=NULL, x = mean_goals-.6, y=1470, label = &quot;mean&quot;, color = &quot;red&quot;) + geom_text(data=NULL, x = median_goals+.75, y=1500, label = &quot;median&quot;, color = &quot;blue&quot;) + geom_text(data=NULL, x = mode_goals-.6, y=1530, label = &quot;mode&quot;, color = &quot;green&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3.1.7 Summary There are multiple measures of central tendency that can be used to summarize an aspect of a distribution: _ (arithmetic) mean, median, and mode_. They answer different questions about distribution. For example, in the distribution of number of goals per game in the previous section mean: “If the same number of goals were scored in each game, how many goals would be scored?” median: “What is a ‘mediocre’ game like?” mode: “What is the most typical game like?” 3.2 Measures of variability The statistics that we’ve discussed so far all relate to central tendency. That is, they all talk about which values are “in the middle” or “popular” in the data. The second thing that we really want is a measure of the variability of the data. That is, how “spread out” are the data? In other words, how ‘representative’ is our measure of central tendency of most data points. Let’s consider interval and ratio scale data. 3.2.1 Range The range of a variable is very simple: it’s the biggest value minus the smallest value. For the sleep data, the maximum value is 20, and the minimum value is 2. We can calculate these values in R using the max() and min() functions: max( mammalian_sleep$sleep_total_h ) ## [1] 20 min( mammalian_sleep$sleep_total_h ) ## [1] 2 where I’ve omitted the output because it’s not interesting. The other possibility is to use the range() function; which outputs both the minimum value and the maximum value in a vector, like this: range( mammalian_sleep$sleep_total_h ) ## [1] 2 20 Although the range is the simplest way to quantify the notion of “variability”, it’s one of the worst. Recall from our discussion of the mean that we want our summary measure to be robust. If the data set has one or two extremely bad values in it, we’d like our statistics not to be unduly influenced by these cases. If we look once again at our toy example of a data set containing very extreme outliers… \\[ -100,2,3,4,5,6,7,8,9,10 \\] … it is clear that the range is not robust, since this has a range of 110, but if the outlier were removed we would have a range of only 8. 3.2.2 Quantiles and percentile A key concept we will need to build on to conceptualize several other measures of variability are quantiles or percentiles. A percentile is the smallest value in a dataset such that a set percentage is smaller than it. (A quantile does pretty much the same but is more generic.) For example, if the 10-th percentile (i.e., the \\(0.1\\) quantile) of a list of values is 73, this means that 10 percent of the values are smaller than or equal to 73. Let’s take a look at the 20 shortest sorted sleep durations and determine the 10-th percentile (\\(0.1\\) quantile), 30-th percentile (\\(0.3\\) quantile), 50-th percentile (\\(0.5\\) quantile), and the 90-th percentile (\\(0.9\\) quantile). Here are the values: sort(mammalian_sleep$sleep_total_h)[1:20] ## [1] 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 6 6 6 6 7 And here is a sorted plot of the 20 smallest values: quantile &lt;- c(0.1,0.3,0.5,0.9) quantile_points &lt;- c(0.1,0.3,0.5,0.9)*20 quantile_labels &lt;- sprintf(&quot;%0.1f quantile\\n(point %d of 20)&quot;, quantile, quantile_points) ggplot(data=NULL, aes(x=1:20, y=sort(mammalian_sleep$sleep_total_h)[1:20])) + geom_point() + geom_vline(xintercept = quantile_points) + geom_label(aes(x=quantile_points, y=6.5, label=quantile_labels )) + scale_x_continuous(breaks = 1:20) + xlab(&quot;Position in ordered vector&quot;) + ylab(&quot;Sleep (hours)&quot;) As you can see: 10-th percentile (\\(0.1\\) quantile): 3 [to be found at position 2, since 2 data points constitute 10 percent of the data] 30-th percentile (\\(0.3\\) quantile): 3 [to be found at position 6, since 6 data points constitute 30 percent of the data] 50-th percentile (\\(0.5\\) quantile): 4 [to be found at position 10, since 10 data points constitute 50 percent of the data] 90-th percentile (\\(0.9\\) quantile): 6 [to be found at position 18, since 18 data points constitute 90 percent of the data] The 50-th percentile is the median. 3.2.3 Interquartile range The interquartile range (IQR) is like the range, but instead of calculating the difference between the biggest and smallest value, it calculates the difference between the 25th quantile and the 75th quantile. R provides you with a way of calculating quantiles, using the (surprise, surprise) quantile() function. Let’s use it to calculate the median sleep durations: quantile( x = mammalian_sleep$sleep_total_h, probs = .5) ## 50% ## 10 And not surprisingly, this agrees with the answer that we saw earlier with the median() function. Now, we can actually input lots of quantiles at once, by specifying a vector for the probs argument. So lets do that, and get the 25th and 75th percentile: quantile( x = mammalian_sleep$sleep_total_h, probs = c(.25,.75) ) ## 25% 75% ## 8 14 And, by noting that \\(14 - 8 = 6\\), we can see that the interquartile range for the sleep durations is 6. Of course, that seems like too much work to do all that typing, so R has a built in function called IQR() that we can use: IQR( x = mammalian_sleep$sleep_total_h ) ## [1] 6 While it’s obvious how to interpret the range, it’s a little less obvious how to interpret the IQR. The simplest way to think about it is like this: the interquartile range is the range spanned by the “middle half” of the data. That is, one quarter of the data falls below the 25th percentile, one quarter of the data is above the 75th percentile, leaving the “middle half” of the data lying in between the two. And the IQR is the range covered by that middle half. 3.2.4 Mean absolute deviation The range and the interquartile range, both rely on the idea that we can measure the spread of the data by looking at the quantiles of the data. However, this isn’t the only way to think about the problem. A different approach is to select a meaningful reference point (usually the mean or the median) and then report the “typical” deviations from that reference point. Let’s go through the mean absolute deviation (AAD for average absolute deviation, since MAD is reserved for the median absolute deviation) from the mean a little more slowly. One useful thing about this measure is that the name actually tells you exactly how to calculate it: \\[ AAD(X) = \\frac{1}{N} \\sum_{i = 1}^N |X_i - \\bar{X}| \\] Let’s compute the AAD for the first data points in the sleep data: \\[ 12, 17, 14, 15, 4 \\] The mean of the dataset is 12.4. That is, \\(\\bar{X} = 12.4\\) The deviations \\(X_i - \\bar{X}\\) are: \\[ -0.4, 4.6, 1.6, 2.6, -8.4 \\] The absolute deviations \\(|X_i - \\bar{X}|\\) are: \\[ 0.4, 4.6, 1.6, 2.6, 8.4 \\] The sum of the absolute deviations \\(\\sum_{i = 1}^N |X_i - \\bar{X}|\\) is 17.6. And \\(N=5\\), which means, that, in our case: \\(AAD(X) = \\frac{1}{N} \\sum_{i = 1}^N |X_i - \\bar{X}| = 3.52\\) In R, we can compute it for the entire vector. mean_sleep &lt;- mean(mammalian_sleep$sleep_total_h) deviation_sleep &lt;- mean_sleep - mammalian_sleep$sleep_total_h mean( abs(deviation_sleep) ) ## [1] 3.576717 An alternative, more compact way to write it is using (lots) pipes: mammalian_sleep$sleep_total_h %&gt;% subtract(., mean(.)) %&gt;% abs() %&gt;% mean() ## [1] 3.576717 The interpretation of the AAD is quite straightforward: It is the average distance from the average. When it’s big, the values are quite spread out. When it’s small, they are close. The units are the same (hours in our case). 3.2.5 Variance Although the mean absolute deviation measure has its uses, it’s not the best measure of variability to use. For a number of practical reasons, there are some solid reasons to prefer squared deviations rather than absolute deviations. A measure of variability based on squared deviations has a number of useful properties in inferential statistics and statistical modeling.3 If we do that, we obtain a measure is called the variance, which for a specific set of observations \\(X\\) is written \\(\\mbox{s}_X^2\\). It is the most wide-spread measure of variability because it is a key concept in inferential statistics. The formula that we use to calculate the variance of a set of observations is as follows: \\[ \\mbox{s}_X^2 = \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 \\] As you can see, it’s basically the same formula that we used to calculate the mean absolute deviation, except that: Instead of using “absolute deviations” we use “squared deviations”. Instead of dividing by \\(N\\) (which gives us the average deviation), we divide by \\(N-1\\) (which gives us ‘sort-of-the-average’). [We will talk about this in a little while.] Now that we’ve got the basic idea, let’s have a look at a concrete example. Once again, let’s use the first five sleep durations. If we follow the same approach that we took last time, we end up with the following table: Table 3.1: Regular, abssolute, and squared deviations Notation [English] \\(i\\) [animal] \\(X_i\\) [value] \\(X_i - \\bar{X}\\) [deviation from mean] \\((X_i - \\bar{X})^2\\) [squared deviation] 1 12 -0.4 0.16 2 17 4.6 21.16 3 14 1.6 2.56 4 15 2.6 6.76 5 4 -8.4 70.56 That last column contains all of our squared deviations, so all we have to do is average them. If we do that by typing all the numbers into R by hand… ( 0.16+21.16+2.56+6.76+70.56 ) / (5-1) ## [1] 25.3 We end up with a variance of 25.3. Exciting, isn’t it? For the moment, let’s ignore the burning question that you’re all probably thinking (i.e., what the heck does a variance of 25.3 actually mean?) and instead talk a bit more about how to do the calculations in R. As always, we want to avoid having to type in a whole lot of numbers ourselves. And as it happens, we have the vector X lying around, which we created in the previous section. With this in mind, we can calculate the variance of X by using the following command, X &lt;- mammalian_sleep$sleep_total_h[1:5] (X - mean(X) )^2 / (length(X) - 1) ## [1] 0.04 5.29 0.64 1.69 17.64 and as usual we get the same answer as the one that we got when we did everything by hand. However, I still think that this is too much typing. Fortunately, R has a built in function called var() which does calculate variances. So we could also do this… var(X) ## [1] 25.3 and you get the same answer. Great. 3.2.6 Standard deviation One problem with the variance is that it is expressed in odd units. In the case above it’s \\(h^2\\) (hours squared). I know what \\(m^2\\), but what are \\(h^2\\)? No idea. Suppose that you’d like to have a measure that is expressed in the same units as the data itself (i.e., points, not points-squared). What should you do? The solution to the problem is obvious: take the square root of the variance, known as the standard deviation, also called the “root mean squared deviation”, or RMSD. This solves out problem fairly neatly. While nobody has a clue what “a variance of 19.95 hours-squared” really means, it’s much easier to understand “a standard deviation of 4.5 hours”, since it’s expressed in the original units. It is traditional to refer to the standard deviation of a sample of data as \\(s_x\\), though “sd” and “std dev.” are also used at times. Because the standard deviation is equal to the square root of the variance, you probably won’t be surprised to see that the formula is: \\[ s_x = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 } \\] \\[ \\hat\\sigma = \\sqrt{ \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 } \\] With that in mind, calculating standard deviations in R is simple: sd( mammalian_sleep$sleep_total_h ) ## [1] 4.466777 Interpreting standard deviations is slightly more complex. Because the standard deviation is derived from the variance, and the variance is a quantity that has little to no meaning that makes sense to us humans, the standard deviation doesn’t have a simple interpretation. As a consequence, most of us just rely on a simple rule of thumb: “in general, you should expect 68% of the data to fall within 1 standard deviation of the mean, 95% of the data to fall within 2 standard deviation of the mean, and 99.7% of the data to fall within 3 standard deviations of the mean”. This rule tends to work pretty well most of the time, but it’s not exact: it’s actually calculated based on an assumption that the histogram is symmetric and “bell shaped”. (Strictly, the assumption is that the data are normally distributed, which is an important concept that we’ll discuss more later). p &lt;- mammalian_sleep %&gt;% ggplot(aes(sleep_total_h)) + geom_histogram(binwidth = 1, color = &quot;black&quot;, fill = &quot;lightgrey&quot;) sleep_sd &lt;- sd(mammalian_sleep$sleep_total_h) sleep_mean &lt;- mean(mammalian_sleep$sleep_total_h) bars &lt;- c(sleep_mean-sleep_sd, sleep_mean, sleep_mean+sleep_sd) bar_labels &lt;- c(&quot;mean-1*sd&quot;, &quot;mean&quot;, &quot;mean+1*sd&quot;) p &lt;- p + geom_vline(xintercept = bars, color = &quot;red&quot;) + geom_label(data=NULL, aes(x=bars[1], y= 10, label = bar_labels[1])) + geom_label(data=NULL, aes(x=bars[2], y= 10, label = bar_labels[2])) + geom_label(data=NULL, aes(x=bars[3], y= 10, label = bar_labels[3])) p Figure 3.3: An illustration of the standard deviation. with(mammalian_sleep, mean(sleep_total_h&gt;(sleep_mean-sleep_sd) &amp; sleep_total_h&lt;(sleep_mean+sleep_sd) )) ## [1] 0.6385542 3.2.6.1 Bessel’s correction: What’s up with all those \\(N-1\\)s in the denominator? Now, what’s going on with that \\(N-1\\), and why do I still call the sample variance a ‘sort-of-the-average’ of the squared deviations? Let’s address these questions in turn. The important thing to note about variance and standard deviation is they serve two purposes: They are used to (i) describe a sample, but also to (ii) tentatively characterize the larger population from which the sample is. You are usually not really interested in the variance of a particular set of numbers, but rather in what they represent. So function number (ii) is the far more dominant use. In our case, when I want to quantify the variability of the sleep durations dataset, it is not these 83 specific mammals I am interested in – I want to get a sense of the variability among mammals in general. That is, I want to know – how much do mammals vary in general. These just happen to be a sample (83 mammals) from the population (all mammals). What I actually want to compute are not the (squared) deviations from the sample mean (\\(\\bar{X}\\); the average sleep duration of these mammals), but from the actual population mean (\\(\\mu\\); the average sleep duration of all mammals). That is, I don’t want \\(( X_i - \\bar{X})^2\\), I want \\(( X_i - \\mu)^2\\). But I don’t know \\(\\mu\\), and the best guess I have about it is \\(\\bar{X}\\). And this has consequences: \\(( X_i - \\bar{X})^2\\) underestimates the distance between \\(X_i\\) and \\(\\mu\\) because we use the same data points (\\(X_i\\)) to compute the mean (\\(\\bar{X}\\)) and then determine the distance to them. Consider what happens to \\(( X_i - \\bar{X})^2\\) in the rather extreme cases of \\(N=1\\)? – The distance is zero. \\(N=2\\)? – If both points happen to be The problem becomes smaller as \\(N\\) increases, because it becomes less and less likely that all \\(N\\) points are squarely on one side of the mean. Dividing by \\(N-1\\) ‘corrects’ this underestimation problem: Dividing by a smaller number makes the estimate of the variance bigger. As \\(N\\) increases the difference between dividing by \\(N\\) and \\(N-1\\) becomes less and less important, and ultimately negligible. 3.2.6.1.1 Summary To recap, these are the two estimators of the variance, but the second one requires knowledge of the true population mean \\(\\mu\\), which we don’t know. Therefore, we use the first one (\\(s_X^2\\)), and divide by \\(N-1\\) to avoid underestimating the ‘true variance’. \\[ \\mbox{s}_X^2 = \\frac{1}{N-1} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right)^2 \\] \\[ \\mbox{Var}_X = \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\mu \\right)^2 \\] output: pdf_document: default html_document: default editor_options: chunk_output_type: console — The choice to use \\(\\Sigma\\) to denote summation isn’t arbitrary: it’s the Greek upper case letter sigma, which is the analogue of the letter S in that alphabet. Similarly, there’s an equivalent symbol used to denote the multiplication of lots of numbers: because multiplications are also called “products”, we use the \\(\\Pi\\) symbol for this; the Greek upper case pi, which is the analogue of the letter P.↩ Or at least, the basic statistical theory – these days there is a whole subfield of statistics called robust statistics that tries to grapple with the messiness of real data and develop theory that can cope with it.↩ I will very briefly mention the one that I think is coolest, for a very particular definition of “cool”, that is. Variances are additive. Here’s what that means: suppose I have two variables \\(X\\) and \\(Y\\), whose variances are \\(\\mbox{Var}(X)\\) and \\(\\mbox{Var}(Y)\\) respectively. Now imagine I want to define a new variable \\(Z\\) that is the sum of the two, \\(Z = X+Y\\). As it turns out, the variance of \\(Z\\) is equal to \\(\\mbox{Var}(X) + \\mbox{Var}(Y)\\). This is a very useful property, but it’s not true of the other measures that I talk about in this section.↩ "],["errorless-lms.html", "Chapter 4 Error-less Linear Models 4.1 Single-variable models 4.2 Multi-variable models 4.3 Models with categorical predictors 4.4 Centered predictors 4.5 Main effects and interactions 4.6 Centered predictors and their effect on main effect and interaction coefficients", " Chapter 4 Error-less Linear Models What I am going to call ‘error-less linear models’ are actually systems of linear equations like those you might recall from school (of the ‘2=a+3x; 1=a+x; solve for a and x’-variety). I’ll call them error-less linear models, because in many important ways they resemble the class of statistical models we will use in this course: (Generalized) Linear Models. In this section we are going to explore error-less linear models, i.e. linear models that work on idealized data, when there is no measurement error, and when the ‘correct’ model describing the data is known. The basic form of a linear regression model, in a form that works for idealized data (i.e., data perfectly follow some hypothesized function): \\[ \\underbrace{Y}_{\\text{dependent variable}} = \\overbrace{\\underbrace{a}_{\\text{intercept}}}^{\\text{additive term}} + \\overbrace{\\underbrace{b_1}_{\\text{slope}} * \\underbrace{X_{1}}_{\\text{predictor}}}^{\\text{additive term}} + \\overbrace{\\underbrace{b_2}_{\\text{slope}} * \\underbrace{X_{2} }_{\\text{predictor}}}^{\\text{additive term}} + \\ldots\\] The typical dataset we will be working with has a structure similar to the one below: dependent variable(\\(Y\\)) predictor 1(\\(X_1\\)) predictor 2(\\(X_2\\)) (further predictors) 2.95 1.1 2.2 (…) 9.1 2.9 -4.0 (…) 5 3.1 -1.4 (…) … … … (…) What the variables represent will depend on the problem you’re studying and the question you’re asking dependent variable (e.g., hours of sleep/day) predictor 1 (e.g., daily food intake in kg) predictor 2 (e.g., deviation from average weight) The dependent variable \\(y\\) is assumed to depend on the predictors \\(x_1, x_2, \\ldots\\). The predictors \\(x_1, x_2, \\ldots\\) can be independent variables (i.e., under experimental control), or other types of covariates (i.e., simply observed). The model is called linear because \\(y\\) is assumed to be a linear function of the \\(x_1, x_2, \\ldots\\). That \\(y\\) increases by some fixed amount \\(\\Delta y\\) for every increase of \\(\\Delta x_i\\) in \\(x_i\\). Two types are usually distinguished: single-variable, and multi-variable (also: simple linear regression, multiple linear regression). The above equation is about vectors of observations. Let’s see how it translates to statements about specific observations. Please note that \\(Y\\), \\(X_1\\), \\(X_2\\) in the equation above are all vectors, while \\(y_i\\), \\(x_{1,i}\\), \\(x_{2,i}\\) are all specific numbers (from the \\(i\\)-th row). \\[ \\underbrace{y_i}_{\\text{dependent variable}} = \\overbrace{\\underbrace{a}_{\\text{intercept}}}^{\\text{additive term}} + \\overbrace{\\underbrace{b_1}_{\\text{slope}} * \\underbrace{x_{1,i}}_{\\text{predictor}}}^{\\text{additive term}} + \\overbrace{\\underbrace{b_2}_{\\text{slope}} * \\underbrace{x_{2,i} }_{\\text{predictor}}}^{\\text{additive term}} + \\ldots\\] For two predictors \\(X_1\\) and \\(X_2\\), and three observerations like in the table above, this equation translates to the following equation system \\[ y_1 = a + b_1\\cdot x_{1,1} + b_2\\cdot x_{2,1} \\\\ y_2 = a + b_1\\cdot x_{1,2} + b_2\\cdot x_{2,3} \\\\ y_3 = a + b_1\\cdot x_{1,3} + b_2\\cdot x_{2,3}\\] That is, \\[ 2.95 = a + b_1\\cdot 1.1 + b_2\\cdot 2.2 \\\\ 9.1 = a + b_1\\cdot 2.9 + b_2\\cdot (-4) \\\\ 5 = a + b_1\\cdot 3.1 + b_2\\cdot (-1.4)\\] In this case, there is a perfect solution: \\(a \\approx 7.90\\), \\(b_1 \\approx -1.60\\), \\(b_2 \\approx -1.45\\) (rounded to two decimal places). \\[ \\underbrace{y}_{\\text{dependent variable}} = \\overbrace{\\underbrace{7.9}_{\\text{intercept}}}^{\\text{additive term}} + \\overbrace{\\underbrace{(-1.60)}_{\\text{slope}} * \\underbrace{x_1}_{\\text{predictor}}}^{\\text{additive term}} + \\overbrace{\\underbrace{(-1.45)}_{\\text{slope}} * \\underbrace{x_2}_{\\text{predictor}}}^{\\text{additive term}} + \\ldots\\] model prediction dependent variable predictor 1 predictor 2 (further predictors) \\(7.9-1.6\\cdot 1.1-1.45\\cdot 2.2\\) 2.95 1.1 2.2 (…) \\(7.9-1.6\\cdot 2.9-1.45\\cdot (-4)\\) 9.1 2.9 -4.0 (…) \\(7.9-1.6\\cdot 3.1-1.45\\cdot (-1.4)\\) 5 3.1 -1.4 (…) … … … (…) Throughout this course, we will make use of the (Generalized) Linear Model as the main statistical model because it is: fairly easy to use (implemented in most programming languages) sufficiently versatile for most practical purposes (can handle most problems you would want to solve at this point) well-studied (all mistakes you will make have already been made, and written about) 4.1 Single-variable models Let’s take a look at a dataset of taxi rides in a country far far away. Imagine that I don’t speak the local language, but I need to go places, and in the process, I would like to understand how the cab fare system works. I have taken 18 taxi rides, and recorded the travel distance, as well as the taxi fare for reach ride. The following plot shows each of these measurements as a point with the distances (in km) on the x-axis, and the fare (money units, MU) on the y-axis. This dataset is interesting, because you probably understand the typical relationships between ride distance and fare fairly well – it is linear. So it is an ideal example for illustrating linear models. Unsurprisingly, we can see clearly that the fare increases with distance. In other words, distance and fare have a positive relationship (if one increases, so does the other). The relationship between the two variables is so strong that if we know the value of one of the two variables for a particular data point, we can predict the value of the other with a high degree of confidence. (You might want to say that I can predict if with absolute certainty, but that presupposes precise knowledge of the cab fare system in that city, and an extreme degree of trust towards the local taxi drivers. Let’s assume that we don’t have either.) We can make that prediction with such a high degree of certainty because all the points seem to lie on a line. If we learned the function that describes this line, we could learn much more from this dataset than the simple fact that the two variables a positive related. That is unsurprising to begin with, but we may want to know how exactly they are related. A simple way to capture this relationship is to try and describe it with this function: \\[ \\text{fare} = a + b \\cdot \\text{distance} \\] What this equation says is that if we take a value for distance, multiply it by some number called \\(b\\), and then add another number called \\(a\\), we will know the fare that corresponds to this distance. (In other words, this equation posits that the relationship between distance and fare is linear.4) In this particular case the relationship is \\(\\text{fare} = 4 + 2.5 \\cdot \\text{distance}\\), which means that \\(a=4\\), and \\(b=2.5\\). We can verify that this equation is indeed the correct generalization by visualizing this function as a line in the plot below: As you can see, it accurately describes all the points in the graph. Importantly, if we conceptualize \\(a\\) and \\(b\\) as just some numbers that we need to add and multiply by, we will miss an important insight: Both numbers have useful interpretations. In this case, \\(a\\) and \\(b\\) can be interpreted as follows: \\(a\\) is called the intercept: It can be interpreted as the value of fare when distance is 0. In this case, the intercept is \\(4~MU\\), which is the amount you have to pay if you get in and change your mind after the taxometer has been switched on (if the taxi driver is a real stickler for rules). \\(b\\) is called the slope for distance: It can be interpreted as the additional amount of money you have to pay for every additional kilometer traveled. A slope of \\(2.5\\) means that a distance increase of \\(1~km\\) increases the fare by \\(2.5~MU\\). The plot below illustrates these interpretations: 4.2 Multi-variable models Let’s imagine that this imaginary city also has many bridges. Since I wasn’t sure whether bridge tolls apply, I also recorded the number of bridges crossed on each trip. In the previous section, we’ve looked at the subset of data where no bridges were crossed. Let’s see how the fare depends not only on distance travelled, but also on the number of bridges crossed. Now, we are looking at the relationship between three variables, and the data frame looks as follows: Let’s see if there even is a relationship. Let’s look at the effect of bridges at distance=\\(3\\,km\\). Yup, it looks like there is a positive effect. Let’s model it in additive fashion: We will again assume that the relationships between fare and distance, as well as between fare and number of bridges crossed is linear, and that they contribute to the fare independently. We can now describe the relationship with the following equation: \\[ fare = a + b_1 \\cdot distance + b_2 \\cdot N_{bridges} \\] The correct parameter values for this equation are \\(a=4\\), \\(b_1 = 2.5\\) (as previously), and \\(b_2=5\\). The interpretation of the coefficients is similar to the previous section: \\(a\\) (the intercept) is the fare when no bridges have been crossed, and the travel distance is zero.5 \\(b_1\\) (the slope for distance) is the additional fare for every additional kilometer. \\(b_2\\) (the slope for \\(N_{bridges}\\)) is the additional fare for every additional bridge. The following plot shows the data vis-à-vis the linear model fits. You can turn the plot in the browser. Drag it around using your mouse. Because we are dealing with a relationship between three variables, every datapoint is a point in three-dimensional space (x-axis: distance, y-axis: number of bridges, z-axis: fare). The multi-variable model fit is illustrated by the blue plane perfectly fitting through all points, which rises with increasing distance and/or number of bridges crossed. The green plane below shows the single-variable model from the last section (\\(fare = 4 + 2.5\\cdot distance\\)) for comparison. The plane corresponding to the single-variable model does not rise with the number of bridges crossed. This is because it is not used as a predictor in that model. You will also notice that the red plane and the black plane intersect in a line at \\(N_{bridges} = 0\\). This is because the two model equations \\(a + b_1 * distance\\) and \\(a + b_1 * distance + b_2 * N_{bridges}\\) are equivalent for \\(N_{bridges} = 0\\). 4.3 Models with categorical predictors Predictors on the nominal or ordinal scale (as opposed to ratio, or interval scale) are not naturally represented by numbers. For example: Speaker gender Presence or absence of wh-movement word order (e.g., SOV, SVO, VSO) size (long, short) We can represent them numerically, but there are many coding options. We can represent a factor with two levels by (i) 0 and 1, or (ii) by -1 and 1, or (iii) by +0.5 and -0.5, or even (iv) 23 and 42. In the case of the taxi fares, one such predictor is the color of the taxi: I’ve observed red and yellow taxis, and I didn’t know if there is possibly a difference in their pricing. In order to determine whether there is, I decided to take a ride in both types. (All the data in the previous sections was for yellow cabs only.) The following plot shows the relationship between fare and cab color (for \\(distance=10~km\\), and \\(N_{bridges} = 0\\)). The x-axis shows the contrast for cab color (the numerical representation of of the categorical variable ‘color’). The y-axis shows the cab fare. While the choice of the coding scheme is somewhat arbitrary, it will affect the interpretation of the coefficients (that is, the parameters of the linear model, i.e., the intercept and the slope). In other words, model parameterization (or: contrast specification) affects the meaning of the coefficients. We will use equation (4.1) to describe the relationship, where cCabColorRed is a numerical representation of the cab color. \\[\\begin{equation} \\text{fare} = a + b \\cdot \\text{cCabColorRed} \\tag{4.1} \\end{equation}\\] We will consider two ways representing color: treatment contrasts, sum contrasts. For the interpretations of the coefficients, keep in mind that we are using a subset of the data with \\(10\\,km\\) rides, when no bridges have been crossed. treatment contrasts sum contrasts contrast coding yellow = 0; red = 1 yellow = -0.5; red = 0.5 \\(a\\) (intercept) interpretation The fare ride in a yellow taxi The average of the fares for a ride in a yellow taxi and one in a red taxi.6 \\(b\\) (slope) interpretation The fare difference between a ride in a red taxi and a yellow taxi. The fare difference between a ride in a red taxi and a yellow taxi. 4.4 Centered predictors Now let’s imagine that I don’t know if the number of passengers affects the fare. So far I’ve been riding alone. Now let’s look at a few rides with several other people (\\(distance = 10~km\\), \\(N_{bridges} = 0\\), yellow cabs only). As previously, we will model the relationship with a linear equation: \\[ \\text{fare} = a + b * \\text{number of passengers} \\] However if we model it like this, our intercept is 24 as you see in the plot below, and not 29, as we would expect. We did travel 10 km, after all. - Why is the fare so low? The reason is that the intercept is the value of the fare when all predictors are 0 - yes, even the number of passengers. Barring any extremely unusual situations, this is not actually possible, and therefore an intercept of this kind is really just a number we need to plug into the equation. In other words, we don’t learn anything from it. In order to remedy the situation and obtain a more useful intercept, we can center the predictor - that is, subtract the average of the vector from the vector itself. In this case, since the average number of passengers is \\(2.5\\), we will use \\[\\text{fare} = a + b * (\\text{number of passengers}-2.5)\\] This will leave us with the following interpretations of the coefficients: \\(a\\): the fare paid for the average number of passengers, in this case for 2.5 passengers (on a 10 km ride, crossing 0 bridges, in a yellow cab) \\(b\\): the additional fare for every additional passenger (on a 10 km ride, crossing 0 bridges, in a yellow cab) As you see, centering the predictor in a single-variable model changes the meaning of the intercept. As you can verify in the above plot, centering makes the intercept correspond to the cab fare at the average number of passengers (instead of at zero passengers). This change in interpretation is because centering changes the interpretation of the zero point of the predictor. While zero corresponds to no passengers when the predictor for the number of passengers is uncentered, it corresponds to the average number of passengers when it is centerered. The data is “moved to the left”, so to say. Because the intercept corresponds to the value of the fare at zero, the intercept changes as a result of centering. In sum, centering predictors in single-variable models can help obtain interpretable coefficients. 4.5 Main effects and interactions Let’s take a look the effect of cab color and cab size. What I didn’t mention before is that cabs come in two different sizes: 5-seaters, and 9-seaters. I want to find out whether larger cabs cost more. We’ll be looking at a subset of data with \\(distance = 10~km\\), and \\(N_{bridges}=0\\), only day rides. We will set up numerical contrasts for the predictors for cab color (yellow and red), and cab size (5-seaters and 9-seaters), and model the relationship with the following linear equation. This means that we are trying to estimate the (main) effects of cab color and size, and maybe something else. As previously, I would like to model the relationship using two additive terms, one for color, one for size. \\[\\begin{equation} \\text{fare} = a + b_1 * \\text{cCabColorRed} + b_2 * \\text{cCabSize9Seater} \\tag{4.2} \\end{equation}\\] Let’s see if that’s possible. Here is a plot of the data along with connecting lines (not a model, just lines). This is weird … the effect of size depends on cab color. (You can see that the lines aren’t parallel, which means that the distance between them changes as x changes.) Here is the raw data. So, can we model the relationship using equation (4.2)? ## cab_color_red cab_size_9seater fare_mnt ## 1126 0 1 44 ## 4113 0 0 29 ## 5832 1 0 39 ## 9756 1 1 59 Of course not, because we need to account for the fact that the effect of color depends on size. In other words, there is an interaction between color and size. As you see, no additive model will describe the relationship correctly. Luckily, it’s only one point that is usually off. So what we need, is to ‘bump up’ the value at \\(size=1, color=1\\). We can do this by creating a new predictor from the old ones, and giving it its own slope: ## cab_color_red cab_size_9seater cab_color_red__by__cab_size_9seater fare_mnt ## 1 0 1 0 44 ## 2 0 0 0 29 ## 3 1 0 0 39 ## 4 1 1 1 59 The new model, which includes two ‘main effects’ and an ‘interaction’ between them becomes: \\[\\begin{equation} \\text{fare} = a + b_1 * \\text{cCabColorRed} + \\\\ b_2 * \\text{cCabSize9Seater} + \\\\ b_3 * \\text{cCabColorRed} * \\text{cCabSize9Seater} \\tag{4.3} \\end{equation}\\] In this model, 5 is added to the prediction when both size, and color equal 1. Such interactions also work quite well for continuous predictors, but that’s for another time … 4.6 Centered predictors and their effect on main effect and interaction coefficients We will set up numerical contrasts for the predictors for cab color (yellow and red), and cab size (5-seaters and 9-seaters), and model the relationship with the following linear equation. This means that we are trying to estimate the main effects of cab color and size, as well as their interaction. \\[ \\text{fare} = a + b_1 * \\text{cCabColorRed} + b_2 * \\text{cCabSize9Seater} + b_3 * \\text{cCabColorRed} * \\text{cCabSize9Seater} \\] We can treat the predictors in one of two ways7: We can either encode them using treatment contrasts (\\(0/1\\)), or using sum contrasts (\\(-.5/+.5\\)). Let’s review in turn what happens when we use each of those contrast coding schemes. 4.6.1 Treatment Contrasts If we use treatment contrasts, the combinations of our predictors can be reduced to the four unique cases below. The first three columns indicate the encoding of the color contrast (yellow=0, red=1) and size contrast (5-seater=0, 9-seater=1), and the interaction predictor (the product of the previous two). The next column illustrates what happens when we plug these predictors into the equation above. In other words, it shows how we can construct the predicted fare from the coefficients of the linear model. Finally, the last column illustrates the simplified version of the predicted value of the fare, with all terms which equal zero removed. This table below illustrates that: The intercept \\(a\\) corresponds to the fare for yellow 5-seaters (it is the only non-zero term in row (1a)). The slope \\(b_1\\) corresponds to the effect of color for 5-seaters only (it is the difference between rows (1c) and (1a)). The slope \\(b_2\\) corresponds to the effect of size for yellow cabs only (it is the difference between rows (1b) and (1a)). The slope \\(b_3\\) corresponds to the additional effect of size for red cabs. Alternatively, it can be understood as the additional effect of color for 9-seaters. (In understanding the above reasoning it may help not to focus on how \\(a,b_1,b_2\\) and \\(b_3\\) are obtained, but rather focus on how \\(a,b_1,b_2\\) and \\(b_3\\) would behave in the above equations if they corresponded to the meaning I assigned them.) color size color \\(\\cdot\\) size (1a) \\(0\\) \\(0\\) \\(0\\) \\(fare = a + 0 \\cdot b_1 + 0 \\cdot b_2 + 0 \\cdot b_3\\) \\(= a\\) (1b) \\(0\\) \\(1\\) \\(0\\) \\(fare = a + 0 \\cdot b_1 + 1 \\cdot b_2 + 0 \\cdot b_3\\) \\(= a + b_2\\) (1c) \\(1\\) \\(0\\) \\(0\\) \\(fare = a + 1 \\cdot b_1 + 0 \\cdot b_2 + 0 \\cdot b_3\\) \\(= a + b_1\\) (1d) \\(1\\) \\(1\\) \\(1\\) \\(fare = a + 1 \\cdot b_1 + 1 \\cdot b_2 + 1 \\cdot b_3\\) \\(= a + b_1 + b_2 + b_3\\) 4.6.2 Centered Contrasts (aka Sum Contrasts) One of the problems with the above coefficient interpretations is that in most cases they are not quite what we are looking for. In most cases, we’ll be interested in the average effect of cab color, the average effect of cab size, and the interaction between the two, and not in the effect of color for small cabs, the effect size for yellow cabs, and the interaction between the two. While we can compute one set of coefficients from the other, the former are far more informative. Why would you want to invest several hours into learning how to do this with a linear model? (Because it’ll save you a lot of headaches later on.) In trying to understand how these contrasts work, please focus on what properties they have rather than on how I (or someone) came up with them. Using sum contrasts will give us more informative coefficients. The table below also illustrates that: The intercept \\(a\\) corresponds to the average fare in the data set (the average of the rows (2a-d) is \\(a\\)). The slope \\(b_1\\) corresponds to the average effect of size (the average of (2a,b) minus the average of (2c,d) is \\(b_1\\)). The slope \\(b_2\\) corresponds to the average effect of color (the average of (2a,c) minus the average of (2b,d) is \\(b_2\\)). The slope \\(b_3\\) corresponds to the additional effect of size for red cabs. Alternatively, it can be understood as the additional effect of color for 9-seaters. color size color \\(\\cdot\\) size (2a) \\(-.5\\) \\(-.5\\) \\(+.25\\) \\(fare = a - .5 \\cdot b_1 - .5 \\cdot b_2 + .25 \\cdot b_3\\) (2b) \\(-.5\\) \\(+.5\\) \\(-.25\\) \\(fare = a - .5 \\cdot b_1 + .5 \\cdot b_2 - .25 \\cdot b_3\\) (2c) \\(+.5\\) \\(-.5\\) \\(-.25\\) \\(fare = a + .5 \\cdot b_1 - .5 \\cdot b_2 - .25 \\cdot b_3\\) (2d) \\(+.5\\) \\(+.5\\) \\(+.25\\) \\(fare = a + .5 \\cdot b_1 + .5 \\cdot b_2 + .25 \\cdot b_3\\) Please note that we don’t neeed to assume that the ‘true relationship’ between the two variables is actually linear. Most interesting relationships are not linear. However, we can still learn a lot about them from a linear model as you will see in the following, especially if we are willing to assume that they are approximately linear. ↩ While the number of bridges depends on distance, it is in principle possible to cross a bridge while keeping the travel distance below a kilometer, in which case it would count as zero.↩ Please note that it is the average of the two fares, and not the average fare. ↩ There are more ways to encode the predictors, but these are the most common ones.↩ "],["lms-with-error.html", "Chapter 5 Linear Models With Error 5.1 A Whole Zoo of Models 5.2 An Updated Linear Model", " Chapter 5 Linear Models With Error So far, we have been working with specific subsets of the data which kept all variables that were irrelevant for our present purposes constant (e.g., \\(distance=10~km\\), \\(N_{bridges} = 0\\)). We were able to do that because we knew which variables are relevant since the dataset was created artificially. In real-life scenarios this is not always possible - not even when describing a deterministic system such as the cab fare pricing scheme. This is because: We often don’t know all the relevant variables. We don’t know in which ways they may interact. Even if we knew all that, we may be left with very little data if we keep taking subsets with specific characteristics all the time. (For example, a dataset with \\(N_{bridges} = 0\\) may not have a lot of rides with \\(distance &gt; 1~km\\) if the city has many bridges.) The bottom line is that in any realistic situation, we will omit relevant predictors from the model specification, because we don’t know what they are, or couldn’t (afford to them) record them. Let’s look at how this would affect our analysis by applying a single-variable model to the data from section 4.2, which varies in distance an number of bridges. We will apply the single-variable model in equation (5.1) to a random(-ish) sample from the data from section 4.2 (i.e., only yellow cabs, one passenger). In a real-world scenario, we could use a model with missing predictors because because we did not consider those predictors relevant, or because we may not have information on them (i.e., we didn’t record the number of bridges on our taxi rides through town). \\[\\begin{equation} \\text{fare} = a + b * \\text{distance} \\tag{5.1} \\end{equation}\\] The plot below is not as clean as the previous plots - and it doesn’t seem to be possible to perfectly describe all the points by one line. While the average fare seems to increase with distance, there is also some variability in the fare at most distances. This is because the number of bridge crossings is unaccounted for. It is as if we were looking at a photo of a three-dimensional plot which was taken from the side. 5.1 A Whole Zoo of Models There is a whole range of models we can use to describe these data. (The numbers next to the data points denote the ‘errors’, i.e., the deviations from the line.) Which would you choose? One way to think about it is – which model has the largest ‘error’? … And how shall we measure the error anyway? – Well, we could use the individual ‘errors’, also called the ‘residuals’. The table below summarizes the sum of absolute errors and the sum of squared errors. As you see, the sum of absolute errors is the same for models 2,3, and 4. And that makes sense, because the sum of 4 errors of magnitude 5, and 4 errors of magnitude 0 is the same as the sum of 8 errors of magnitude 2.5. The sum of squared errors on the other hand is the lowest for model 4. This is because the sum of squared errors tends to be bigger for combinations of one big error and one small error (\\(5^2+0^2=25\\)), than for two medium-sized errors (\\(2.5^2+2.5^2 = 12.5\\)). The sum of squared errors seems to match our intuition about model quality. Let’s use it for now. Sum of absolute errors Sum of squared errors model 1 25 112.5 model 2 20 100 model 3 20 100 model 4 20 50 Interesting, so different sets of parameters result in different sums of squared errors. If we could only create some sort of map of it in order to look at all the other options and find the best parameters (those that minimize that error). It turns out, we can. What you see below is a plot of the sum of squared errors (y-axis), as a function of the values for the intercept \\(a\\) (x-axis) and slope (z-axis). The red point marks parameter combination with the lowest error (50). The key insight the plot should provide is that having a function which allows us to compare different models (also known as the objective function) allows us to select the best model, because all we have to do is find its minimum (i.e., the combination of intercept and slope with the smallest sum of squared errors). 5.2 An Updated Linear Model In the previous chapter, our system was deterministic since we knew every every variable that mattered and how it was to be included into the linear model equation. Once we fail to account for some predictors that matter, the system becomes non-deterministic - we can’t predict the fare exactly. In this case, this means that we can’t predict the exact fare, if we don’t know how many bridges were crossed on that taxi ride. Unlike previously, we can’t solve a set of equations to arrive at coefficients that describe the data perfectly. But what we can do, is choose the coefficients such that the fare is predicted as well as possible. While we can’t account for all variation in the fare, we can at least account for some of the structure in it - and all the plots show that there is some structure. In order to account for omitted variables (or possibly true randomness in the data), we need to modify the linear model equation to the form below. \\[ \\underbrace{y_i}_{\\text{Observed value}} = \\overbrace{\\underbrace{a}_{\\text{Intercept}}}^{\\text{additive term}} + \\overbrace{\\underbrace{b_1}_{\\text{Slope}} * \\underbrace{{x_1}_i}_{Predictor}}^{\\text{additive term}} + \\overbrace{\\underbrace{b_2}_{\\text{Slope}} * \\underbrace{{x_2}_i}_{Predictor}}^{\\text{additive term}} + \\ldots + \\underbrace{\\epsilon_i}_{Error}\\] The new element here is the error term \\(\\epsilon\\), while the rest of the equation is as previously. You can think of the equation as consisting of three components. The intercept accounts for all constants, as well as the average effect of the omitted (and possibly even unknown predictors). The slopes account for all variable effects due to known predictors. Meanwhile, the error \\(\\epsilon\\) term accounts for all variable effects due to omitted predictors. All errors need to sum to zero. (If they didn’t, that part should go to the intercept.) "],["examples.html", "Chapter 6 Examples 6.1 The Dative Verbs Data", " Chapter 6 Examples 6.1 The Dative Verbs Data I want to know if the length of the theme and the recipient influence the proportion of recipient-theme word orders, and by how much. (In this sample!) Let’s prepare the data. # load all packages that will be required library(dplyr) library(magrittr) library(ggplot2) library(languageR) # create a data frame with the required columns dative_relevant &lt;- dative %&gt;% dplyr::select(RealizationOfRecipient, LengthOfRecipient, LengthOfTheme) # take a look head(dative_relevant, 3) ## RealizationOfRecipient LengthOfRecipient LengthOfTheme ## 1 NP 1 14 ## 2 NP 2 3 ## 3 NP 1 13 # discretize length of recipient dative_relevant %&lt;&gt;% mutate( cat_LengthOfRecipient = ifelse(LengthOfRecipient &gt; median(LengthOfRecipient) , &quot;long&quot;, &quot;short&quot;) %&gt;% as.factor() ) dative_relevant %&lt;&gt;% mutate( cat_LengthOfTheme = ifelse(LengthOfTheme &gt; median(LengthOfTheme) , &quot;long&quot;, &quot;short&quot;) %&gt;% as.factor() ) 6.1.1 By Length of Recipient Now, let’s summarize and plot it # compute percentage of default word orders by categorical recipient length (dative_summary1 &lt;- dative_relevant %&gt;% group_by(cat_LengthOfRecipient) %&gt;% summarize(perc_def = mean(RealizationOfRecipient == &quot;NP&quot;), N = n())) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 3 ## cat_LengthOfRecipient perc_def N ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 long 0.448 1035 ## 2 short 0.875 2228 # plot averages ggplot(dative_summary1, aes(x= cat_LengthOfRecipient, perc_def)) + geom_point() Now, let’s fit a mode with treatment contrasts and look at its coefficients. # Fit model 1A (treatment contrasts): # Fit a linear model with treatment contrasts according to the following specification: # perc_def = a + b*length of recipient # create a treatment contrast for (categorical) length (&#39;ct&#39; is for &#39;contrast, treatment&#39;) dative_summary1 %&lt;&gt;% mutate(ct_LR = dplyr::recode(cat_LengthOfRecipient, &quot;short&quot;=0, &quot;long&quot;=1)) # use coef(lm(...)) to fit the linear model described above and extract its coefficients coef(lm( perc_def ~ 1 + ct_LR, data = dative_summary1)) ## (Intercept) ct_LR ## 0.8752244 -0.4269152 Now, let’s fit a mode with sum contrasts and look at its coefficients. # Fit model 1B (sum contrasts): # Fit a linear model with sum contrasts according to the following specification: # perc_def = a + b*length of recipient # create a treatment contrast for (categorical) length (&#39;cs&#39; is for &#39;contrast, sum&#39;) dative_summary1 %&lt;&gt;% mutate(cs_LR = dplyr::recode(cat_LengthOfRecipient, &quot;short&quot;=-.5, &quot;long&quot;=.5)) # use coef(lm(...)) to fit the linear model described above and extract its coefficients coef(lm( perc_def ~ 1 + cs_LR, data = dative_summary1)) ## (Intercept) cs_LR ## 0.6617668 -0.4269152 6.1.2 By Length of Theme Now, let’s summarize and plot the averages. # compute percentage of default word orders by categorical theme length (dative_summary2 &lt;- dative_relevant %&gt;% group_by(cat_LengthOfTheme) %&gt;% summarize(perc_def = mean(RealizationOfRecipient == &quot;NP&quot;), N = n())) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 3 ## cat_LengthOfTheme perc_def N ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 long 0.826 1309 ## 2 short 0.682 1954 # plot averages ggplot(dative_summary2, aes(x= cat_LengthOfTheme, perc_def)) + geom_point() Now, let’s fit a mode with treatment contrasts and look at its coefficients. # Fit model 2A (treatment contrasts): # Fit a linear model with treatment contrasts according to the following specification: # perc_def = a + b*length of theme # create a treatment contrast for (categorical) length (&#39;ct&#39; is for &#39;contrast, treatment&#39;) dative_summary2 %&lt;&gt;% mutate(ct_LT = dplyr::recode(cat_LengthOfTheme, &quot;short&quot;=0, &quot;long&quot;=1)) # use coef(lm(...)) to fit the linear model described above and extract its coefficients coef(lm( perc_def ~ 1 + ct_LT, data = dative_summary2)) ## (Intercept) ct_LT ## 0.6821904 0.1436309 Now, let’s fit a mode with sum contrasts and look at its coefficients. # Fit model 1B (sum contrasts): # Fit a linear model with sum contrasts according to the following specification: # perc_def = a + b*length of recipient # create a treatment contrast for (categorical) length (&#39;ct&#39; is for &#39;contrast, treatment&#39;) dative_summary2 %&lt;&gt;% mutate(cs_LR = dplyr::recode(cat_LengthOfTheme, &quot;short&quot;=-.5, &quot;long&quot;=.5)) # use coef(lm(...)) to fit the linear model described above and extract its coefficients coef(lm( perc_def ~ 1 + cs_LR, data = dative_summary1)) ## (Intercept) cs_LR ## 0.6617668 -0.4269152 6.1.3 By Length of Recipient and Length of Theme Now, let’s summarize and plot the averages. # compute percentage of default word orders by categorical theme and recipient length (dative_summary3 &lt;- dative_relevant %&gt;% group_by(cat_LengthOfTheme, cat_LengthOfRecipient) %&gt;% summarize(perc_def = mean(RealizationOfRecipient == &quot;NP&quot;), N = n() )) ## `summarise()` regrouping output by &#39;cat_LengthOfTheme&#39; (override with `.groups` argument) ## # A tibble: 4 x 4 ## # Groups: cat_LengthOfTheme [2] ## cat_LengthOfTheme cat_LengthOfRecipient perc_def N ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 long long 0.616 469 ## 2 long short 0.943 840 ## 3 short long 0.309 566 ## 4 short short 0.834 1388 # plot averages dative_summary3 %&gt;% ggplot(aes( cat_LengthOfRecipient, perc_def, color = cat_LengthOfTheme, group = cat_LengthOfTheme)) + geom_point() + geom_line() Now, let’s fit a mode with treatment contrasts and look at its coefficients. # Fit model 2A (treatment contrasts): # Fit a linear model with treatment contrasts according to the following specification: # perc_def = a + b1*length of theme + b2*length of recipient + b3*length of theme*length of recipient # create a treatment contrast for (categorical) length (&#39;ct&#39; is for &#39;contrast, treatment&#39;) dative_summary3 %&lt;&gt;% mutate( ct_LR = dplyr::recode(cat_LengthOfRecipient, &quot;short&quot;=0, &quot;long&quot;=1), ct_LT = dplyr::recode(cat_LengthOfTheme, &quot;short&quot;=0, &quot;long&quot;=1) ) # use coef(lm(...)) to fit the linear model described above and extract its coefficients coef(lm( perc_def ~ 1 + ct_LT + ct_LR + ct_LT:ct_LR, data = dative_summary3)) ## (Intercept) ct_LT ct_LR ct_LT:ct_LR ## 0.8342939 0.1085632 -0.5251067 0.1984542 Please verify the meaning of the coefficients in the above plots, data frames and other models Intercept: The perc_def value for the baseline value (i.e., short theme and short recipient). Main effect, coefficient for ct_LT: The effect of theme length when the recipient is short. Main effect, coefficient for ct_LR: The effect of recipient length when the theme is short. Interaction, coefficient for ct_LT:ct_LR: The additional effect of theme length when the the recipient is long. (or: The additional effect of recipient length when the the theme is long.) Now, let’s fit a mode with sum contrasts and look at its coefficients. # Fit model 1B (sum contrasts): # Fit a linear model with sum contrasts according to the following specification: # perc_def = a + b1*length of theme + b2*length of recipient + b3*length of theme*length of recipient # create a sum contrast for (categorical) length (&#39;cs&#39; is for &#39;contrast, sum&#39;) dative_summary3 %&lt;&gt;% mutate( cs_LR = dplyr::recode(cat_LengthOfRecipient, &quot;short&quot;=-.5, &quot;long&quot;=.5), cs_LT = dplyr::recode(cat_LengthOfTheme, &quot;short&quot;=-.5, &quot;long&quot;=.5) ) # use coef(lm(...)) to fit the linear model described above and extract its coefficients coef(lm( perc_def ~ 1 + ct_LT + ct_LR + ct_LT:ct_LR, data = dative_summary3)) ## (Intercept) ct_LT ct_LR ct_LT:ct_LR ## 0.8342939 0.1085632 -0.5251067 0.1984542 Please verify the meaning of the coefficients in the above plots, data frames and other models: Intercept: Average percentage of default word orders. Main effect, coefficient for ct_LT: Average effect of length of the theme. Main effect, coefficient for ct_LR: Average effect of length of the recipient. Interaction, coefficient for ct_LT:ct_LR: Change in the effect of ct_LR due to one unit of change in ct_LT. (or: Change in the effect of ct_LT due to one unit of change in ct_LR.) "],["samples-populations-and-probability.html", "Chapter 7 Samples, Populations and Probability 7.1 The Dative Verbs Data Revisited 7.2 Populations and Samples 7.3 The Role of Statistical Models 7.4 Probability", " Chapter 7 Samples, Populations and Probability 7.1 The Dative Verbs Data Revisited Remember our friend below? \\[ \\underbrace{y_i}_{\\text{Observed value}} = \\overbrace{\\underbrace{a}_{\\text{Intercept}}}^{\\text{additive term}} + \\overbrace{\\underbrace{b_1}_{\\text{Slope}} * \\underbrace{{x_1}_i}_{Predictor}}^{\\text{additive term}} + \\overbrace{\\underbrace{b_2}_{\\text{Slope}} * \\underbrace{{x_2}_i}_{Predictor}}^{\\text{additive term}} + \\ldots + \\underbrace{\\epsilon_i}_{Error}\\] In the last few weeks we talked about linear models and how we can use ther coefficients (intercept and slopes) to characterize our findings in a particular sample. We also discussed the presence of errors. When a linear model can describe the data perfectly, such as in the dative data set, or the taxi data when all predictors are known, we can can find coefficients that reduce the error term to 0. We can interpret the result as an exact characterization of the dependent variable for any combination of predictor values. When a linear model cannot describe the data perfectly because some predictors aren’t known, we can find coefficients that minimize the error term (but don’t make it zero). We can interpret the result as an characterization of the average of the dependent variable for any combination of predictor values. This is all nice. But what do all those fancy numbers like averages and coefficients mean beyond a characterization of the sample? Let’s compare the difference between animate and inanimate in the full data set to the effect in the 10, 50, 100, 500 first rows corresponding to each class (animate and inanimate; i.e., 20, 100, 200, 1000 rows in total). library(languageR) # average over all instances df1 &lt;- dative %&gt;% group_by(AnimacyOfRec) %&gt;% summarize(perc_def = mean(RealizationOfRecipient==&quot;NP&quot;)) p1 &lt;- df1 %&gt;% ggplot(aes(AnimacyOfRec, perc_def)) + geom_bar(stat=&quot;identity&quot;) # average over the first N instances df2 &lt;- dative %&gt;% group_by(AnimacyOfRec) %&gt;% summarize(perc_def = mean(RealizationOfRecipient[1:10]==&quot;NP&quot;)) p2 &lt;- df2 %&gt;% ggplot(aes(AnimacyOfRec, perc_def)) + geom_bar(stat=&quot;identity&quot;) df3 &lt;- dative %&gt;% group_by(AnimacyOfRec) %&gt;% summarize(perc_def = mean(RealizationOfRecipient[1:50]==&quot;NP&quot;)) p3 &lt;- df3 %&gt;% ggplot(aes(AnimacyOfRec, perc_def)) + geom_bar(stat=&quot;identity&quot;) df4 &lt;- dative %&gt;% group_by(AnimacyOfRec) %&gt;% summarize(perc_def = mean(RealizationOfRecipient[1:100]==&quot;NP&quot;)) p4 &lt;- df4 %&gt;% ggplot(aes(AnimacyOfRec, perc_def)) + geom_bar(stat=&quot;identity&quot;) ggarrange(NULL, p1+ggtitle(&quot;all observations&quot;), NULL, p2+ggtitle(&quot;N=10/group&quot;), p3+ggtitle(&quot;N=50/group&quot;), p4+ggtitle(&quot;N=100/group&quot;), ncol = 3, nrow =2) The differences between 0.28 (all), 0 (N=10), 0.32 (N=50), 0.53 (N=100). Which one is ‘right’? The key to answering this question is in understanding what we mean by ‘right’. The brief answer is that all of them are probably wrong, but the ones with bigger \\(N\\)s are more likely to be closer to the ‘truth’. 7.2 Populations and Samples In most research common in (psycho-)linguistics, we aren’t really interested in the particular sample as such. We are interested in using it to find out something more general. Population: The group to which we wish to generalize. We will refer to chracterizations of the population as parameters (\\(\\theta\\), \\(\\alpha\\), \\(\\beta\\), …). The research question is typically about the population. Sample: A subset of the population. We will refer to chracterizations of a sample as statistics (\\(\\widehat{\\theta}\\), \\(\\widehat{\\alpha}\\), \\(\\widehat{\\beta}\\), …). Our data is usually a sample of that population. The expression popularized by Mark Twain “There are three kinds of lies: lies, damned lies, and statistics.” refers to exactly these kinds of statistics, not the scientific discipline of statistics. Statistical Inference Statistical inference is about making inferences about parameters, based on statistics. In our dative data set, our main interest is not in whether the animacy of the recipient is associated with … … a change in the percentage of the realization of default word order in the sample (\\(\\widehat{P_{def,+anim}}\\), \\(\\widehat{P_{def,-anim}}\\)) … … a change in the probability of the realization of default word order, in general (\\(P_{def,+anim}\\), \\(P_{def,-anim}\\)) It is key to understand that they are not the same, but that they are related. 7.2.1 Examples Let’s consider the following research questions, how we would go about solving them, and what would constitute the statistics and the population parameters in these cases. Does speed-reading work? (Note: It doesn’t. Not at all.) Is the buttered toast phenomenon really true? Does topicality of the object increase the likelihood of OVS word order in German? Do taller people have deeper voices? 7.3 The Role of Statistical Models So, we’ve seen that we’re actually interested in population parameters. Great. How do we find out what they are? In order to do that, we’ll first need to understand the relationship between statistics and population parameters, and this is exactly statistical models are for. But in order to understand statistical models, we will first need to talk about probability. 7.4 Probability 7.4.1 Examples of Probabilistic Statments A fair coin will come up heads 50% of the time. The probability of rain tomorrow is 30%. It is very unlikely that I’ll fail that exam. The probability of throwing a 1 with a (fair) six-sided die is 1/6. 7.4.2 The Notion of Probability It is difficult to exactly define probability without using synonyms, although most of us have some sort of intuition of what it is. It is a number that follows certain laws, which could be argued to encode common sense applied to numbers when the system producing them is not predictable, or doesn’t seem to be predictable with our current state of knowledge. 7.4.2.1 1. Classical Probability Equal probabilities assigned to theoretically equally likely events (e.g., heads or tails). Probability of more complex events defined by ‘the number of ways’ that lead to their realization. Example: What is the probability of a fair coin, tossed twice, will come up heads exactly once? There are two equi-probable ways in which this can happen the total number of events that can occur is 4, and so the probability is 0.5. Works well for simple examples (marbles, dice, cards, etc), where the set of all possible events can be decoposed into equally likely events. For example: ‘probability of a coin coming up heads, then tails’ ‘probability of throwing a 1,2, and a 3 in succession with a six-sided die’ Much more difficult to apply to more complex ones. For example: ‘probability of rain on two successive days’ (what could the equally likely events be, and how do they change from day to day?) 7.4.2.2 2. Frequentist Probability Probabilies are (relative) frequencies of events in hypothetical, infinite sequences of experiments (situations in which events may or may not occur). P(A), the probability of an event A, is the proportion of times that A occurs in such a sequence. Importantly, it’s going to be the same proportion every time. In other words, an event’s has a probability of occuring impacts its long-term frequency. Example: ‘probability of a biased coin coming up heads (\\(p_H=0.6\\))’, ‘probability of rain on two successive days’ How do we deal with ‘the probability of rain tomorrow’, or ‘probability that Donald Trump will be re-elected’, or ‘the probability that this mushroom is poisonous’? 7.4.2.3 3. Bayesian Probability Both of the previous conceptualizations of probability are objective: they are about ‘a true state of the world’. The last examples do not really have a probability under those interpretations. (Unless you subscribe to a multiverse-interpretation.) They are either true or false. Yet these expressions somehow make sense to us. We can also conceptualize probability as a degree of belief. Sounds silly? - We use it in that way all the time. For example, in 2004-2005, a group of experts estimated the probability of the deployment of a nuclear weapon in the next 10 years to be \\(50\\)%. What did they mean? We may differ in the probabilities that we assign to an event, based on our state of knowledge about the problem. 7.4.3 The Laws of Probability Whatever interpretation we assign probabilities, they must follow certain laws in order to be useful. It could be argued that the laws essentially to encode common sense applied to numbers. For any event A, the probability of A, written as P(A), is a number between 0 and 1. Law of total probability: The probabilities of all possible events must sum to to 1. Law of complements: The probabilities of an event occuring and an event not occuring must sum 1. 7.4.3.1 Examples … … … 7.4.3.2 Independent events Independent events: Events for which the probability of the occurrence of one of them (\\(A_1\\)) does not depend on whether or not the other one (\\(A_2\\)) has occurred or will occur. For two independent events \\(A_1\\) and \\(A_2\\), the probability of both events occurring (the joint probability of \\(A_1\\) and \\(A_2\\)) is the product of their probabilities: \\[P(A_1, A_2) = P(A_1) \\cdot P(A_2)\\] 7.4.3.3 Examples A fair coin coming first heads, then tails. Rain in Istanbul, and Rain in Sydney … can you think of other examples? 7.4.3.4 Mutually exclusive events Mutually exclusive events: Events that cannot happen at the same time. Can be a logical contradiction, like a 6-sided die coming up with an even number (\\(A_1\\)) and coming up ‘3’ (\\(A_2\\)). Events can also be mutually exclusive without being a logical contradiction: For example, reporting feeling happy and sad at the same time. (But mutually exclusive events will always be in contradiction vis á vis the system that generates the events.) Additive law of probability: For two any events \\(A_1, A_2\\), the probability that one of the events occurs is: \\[P(A_1~or~A_2) = P(A_1) + P(A_2) - P(A_1~and~A_2)\\] Special case: For two mutually exclusive events \\(A_1, A_2\\) the probability that one of the events occurs is the sum of their probabilities: \\[P(A_1~or~A_2) = P(A_1) + P(A_2)\\] 7.4.3.5 Examples A coin coming up heads and tails at the same time. Heavy rain in Istanbul for more than a day, and no flooding in Istanbul … can you think of other examples? 7.4.3.6 Conditional probability Conditional probability: The conditional probability of event \\(A_1\\) given event \\(A_2\\) is the probability that event \\(A_1\\) will occur (or has occured), given that \\(A_2\\) has occured (or will occur). Definition of conditional probability: For two events, the conditional probability \\(P(A_1|A_2)\\) is \\(P(A_1,A_2) / P(A_2)\\). Special case 1: For two independent events, the conditional probability \\(P(A_1|A_2) = P(A_1)\\). Special case 2: For mutually exclusive events, the conditional probability \\(P(A_1|A_2) = 0\\). 7.4.3.7 Examples The probability of having a disease given that a test says so The probability of a test saying that you have a disease given that you do "],["statistical-models.html", "Chapter 8 Statistical Models 8.1 Log-likelihood and numerical underflow", " Chapter 8 Statistical Models 8.0.1 A First Statistical Model (This will be a somewhat clumsy model, and we’ll develop a better one later.) The main aim of a generative model is to describe how the data we observed is generated. Let’s create a simple model of observed word order, with the following assumptions - It follows from this model that the following observations have the following probabilities: recipient observation  (default word order?) probability animate \\(1\\) \\(p_1\\) animate \\(0\\) \\(1-p_1\\) inanimate \\(1\\) \\(p_2\\) inanimate \\(0\\) \\(1-p_2\\) Doesn’t this mean that we could even compute the is the probability of the data, if we only knew \\(p_1\\), and \\(p_2\\)? Absolutely! Lets call our vector of observations \\((y_1, y_2, y_3, \\ldots, y_n)\\). Given specific values for the parameters \\(p_1, p_2\\), the probability of each individual observation of \\(P(y_n|p_1,p_2)\\) can be read is given in the above table. If we can compute the probability of one observation, we should be able to compute the probability of a whole data set. Remember the joint probability of two independent events \\(A\\), \\(B\\)? It’s \\(P(A) \\cdot P(B)\\). … and because our generative process described by the graphic above assumes independence of observations (i.e., it doesn’t assume dependence), we can compute the the probability of the data set as: \\[P(y_1,~y_2,~...,~y_n|p_1,~p_2) = P(y_1|p_1, p_2) \\cdot P(y_2|p_1, p_2)\\cdot ... \\cdot P(y_n|p_1,p_2)\\] Let’s write an R function which will compute the probability of each data point, and one that will compute the probability of the data set. compute_prob_observation &lt;- function(is_rec_animate, is_default_wo, p1, p2) { # make sure we&#39;re dealing with a single observation, i.e., vector length == 1 stopifnot(length(is_default_wo) == 1 &amp;&amp; length(is_rec_animate) == 1) # different courses of action for animate and inanimate recipients if (is_rec_animate) { # I&#39;ll use if/else, but nested ifelse() calls will work just as well prob_of_observation &lt;- ifelse(is_default_wo, p1, 1-p1) } else { # i.e., if is_rec_animate is FALSE (i.e., !is_rec_animate is TRUE) prob_of_observation &lt;- ifelse(is_default_wo, p2, 1-p2) } prob_of_observation # this what the function returns } Let’s test it: compute_prob_observation(is_rec_animate = 1, is_default_wo = 1, p1 = .1, p2 = .7) ## [1] 0.1 OK. compute_prob_observation(is_rec_animate = 1, is_default_wo = 0, p1 = .1, p2 = .7) ## [1] 0.9 OK. compute_prob_observation(is_rec_animate = 0, is_default_wo = 1, p1 = .1, p2 = .7) ## [1] 0.7 OK. compute_prob_observation(is_rec_animate = 0, is_default_wo = 0, p1 = .1, p2 = .7) ## [1] 0.3 OK. It seems to do what it’s supposed to. Now let’s write a function that does it for two vectors, one vector of observations and one of animacy values: # We&#39;ll need to use sapply, which runs through a vector of elements and calls a function for each element # For example: sapply(1:10, print) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 ## [1] 1 2 3 4 5 6 7 8 9 10 compute_probs_all_observations &lt;- function(is_rec_animate, is_default_wo, p1, p2) { # make sure the input vectors are of equal length stopifnot(length(is_default_wo) == length(is_rec_animate) ) # create a function which computes the probability of the i-th observation compute_prob_ith_observations &lt;- function(i) { compute_prob_observation(is_rec_animate = is_rec_animate[i], is_default_wo = is_default_wo[i], p1 = p1, p2 = p2) } # iterate over all positions in the vectors and compute the probability of # the i-th observation on each iteration # (sapply iterates over a vector) sapply(1:length(is_default_wo), compute_prob_ith_observations) } Let’s try it out on some artificial data: # Let&#39;s take a look at the output of the function, with (rather extreme) made-up data, ... # ... sample size 10 per group n &lt;- 5 # 10 1-s, and 10 0-s is_rec_animate &lt;- c(rep(1,n), rep(0,n)) # only default word orders for animate recipients, only non-default for the others is_default_wo &lt;- c(rep(1,n), rep(0,n)) synth_df &lt;- data.frame(is_rec_animate, is_default_wo) View(synth_df) # compute the probability for the first probs &lt;- compute_probs_all_observations(is_rec_animate, is_default_wo, p1 = .5, p2 = .5) How likely are such extreme data under \\(p_1=0.5\\) and \\(p_2=0.5\\)? # are data like this unlikely under p1=0.5 and p2=0.5? probs ## [1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 Not very unlikely. What about the entire dataset? prod(probs) ## [1] 0.0009765625 Quite “unlikely”, but that’s because many numbers \\(&lt;1\\) were multiplied. Would other parameters do any better? Apparently some values for \\(p_1\\) and \\(p_2\\) are better than others. Can you tell from the plot what they are? 8.1 Log-likelihood and numerical underflow Now, let’s apply this logic to the entire dataset. First, let’s check for \\(p_1=0.5\\) and \\(p_2=0.5\\). library(languageR) # bring the observations into a usable format dative %&lt;&gt;% mutate(is_default_wo = (RealizationOfRecipient == &quot;NP&quot;) ) dative %&lt;&gt;% mutate(is_rec_animate = (AnimacyOfRec == &quot;animate&quot;) ) # compute the probability for the first probs &lt;- compute_probs_all_observations(dative$is_rec_animate, dative$is_default_wo, p1 = .5, p2 = .5) prod(probs) ## [1] 0 Zero? Whaaaaaaaat is going on? Might the fact below be relevant? nrow(dative) ## [1] 3263 The product of many numbers between \\(0\\) and \\(1\\) is going to be practically \\(0\\): \\(0.1\\cdot0.1=0.01\\), \\(0.1\\cdot0.1\\cdot0.1=0.001\\), \\(0.1\\cdot0.1\\cdot0.1\\cdot0.1=0.0001\\). This results in numerical underflows. (Computers only have a limited precision.) 8.1.1 Logarithm Only logarithms can help us now: \\(log_b(x) = y\\), when \\(b^y = x\\) (with some details omitted) In other words, the logarithm of \\(x\\) to base \\(b\\) is \\(y\\), which is the number by which you need to exponentiate \\(b\\) to get \\(x\\) For example, \\(log_{10}(100) = 2\\), and \\(10^2 = 100\\). Makes sense? So how will this help us? The logarithm maps numbers between \\(0\\) and \\(1\\) onto the range \\(-\\infty\\) and \\(0\\), i.e. from \\((0;1]\\) to \\((-\\infty; 0]\\). Thus, we can use half the number line just to express numbers between \\(0\\) and \\(1\\) (i.e., probabilities). Multiplication of regular probabilities corresponds to addition of log-probabilities: \\[log_b(x \\cdot y) = log_b(x) + log_b(y)\\] Let’s take a look at this function (\\(log_{e}\\)): 8.1.2 Log-likelihood All this means that we can get the log-probability of the entire dataset by summing all individual log-probabilities \\[log_b(p_1 \\cdot p_2 \\cdot \\ldots \\cdot p_n ) = log_b(p_1) + log_b(p_2) + \\ldots log_b(p_n)\\] Let’s use this to compute a log-likelihood surface for \\(p_1\\) and \\(p_2\\) for the dative data set. The red point marks the maximum likelihood (i.e., the ‘peak’ likelihood). Note that the likelihood profile is much more flat along the \\(p_2\\) axis. Below is are cross-sections of the above plot, with the red point marking the maximum likelihood estimates for \\(p_1\\) and \\(p_2\\). Let’s take a look at why the likelihood profile for \\(p_2\\) is more flat than for \\(p_1\\): It’s an issue of sample size. There are many fewer instances of inanimate recipients (first row) than animate recipients (second row). The flat likelihood profile for \\(p_2\\) tells us that while the parameter which is the most likely to produce the observed data is \\(0.48\\) (which is also the observed proportion \\(\\hat{p_2}\\)), the data isn’t all that much less likely under other values for \\(p_2\\) (for example, \\(0.45\\), \\(.5\\), \\(.52\\)). is_rec_animate perc_default_wo N FALSE 0.4811715 239 TRUE 0.7602513 3024 Cool. So what can we do with this? Well, for one, we can compute the relative likelihood of the data for two sets of parameters. Remember that what we have plotted above is the log-likelihood of the data given the parameters, i.e. \\(log~P(D|p_1, p_2)\\) This means that: \\[P(D|p_1=\\theta_1, p_2=\\theta_1)/P(D|p_1=\\theta&#39;_1, p_2=\\theta&#39;_1),\\] because \\(e^{log(a)} = a\\), since \\(log(x)\\) and \\(e^x\\) are inverses of each other: \\[e^{log( P(D|p_1=\\theta_1, p_2=\\theta_1)/P(D|p_1=\\theta&#39;_1, p_2=\\theta&#39;_1) )},\\] and because \\(log(a/b) = log(a) - log(b)\\): \\[e^{log( P(D|p_1=\\theta_1, p_2=\\theta_1)) - log(P(D|p_1=\\theta&#39;_1, p_2=\\theta&#39;_1) )}\\] Great, now we can quantify for any parameter value \\(\\theta&#39;\\), how much less likely the data are for that parameter value, than for the maximum likelihood estimate \\(\\hat{\\theta}\\). We can even calculate an interval around \\(\\hat{\\theta}\\), such that all \\(\\theta&#39;\\)s for which the data is considered too unlikely, falls outside of it. For a ratio of \\(0.5\\), that interval is roughly \\([.75;.77]\\) for \\(p_1\\), and \\([.44;.52]\\) for \\(p_2\\). But instead, we could ask ourselves, is \\(P(D|p_1,p_2)\\) really what we want? Do we really want the probability of the data given certain parameters? In my opinion, it’s not. We are actually interested in \\(P(p_1, p_2|D)\\). And \\(P(D|p_1,p_2)\\) is just a crude proxy for it. The problem is that one cannot maintain a frequentist view of probability and use the expression \\(P(p_1, p_2|D)\\) … "],["statistical-models-ii.html", "Chapter 9 Statistical Models II 9.1 Probability Distributions 9.2 Binomial Distribution 9.3 A likelihood function for the vaccine data 9.4 Bayesian Inference I 9.5 Returning to Example 1 9.6 Bayesian Inference II 9.7 Back to our incidence rates", " Chapter 9 Statistical Models II Sometimes we have data in a format that doesn’t allow us a to formulate a generative model that goes all the way. 9.0.1 Example 1 For example here is data from two SinoVac phase-III trials (for the ‘CoronaVac’ vaccine). [Data: Jan 15, 2021] country group total participants COVID-19 cases incidence (estimated rate) estimated vaccine efficacy Brazil placebo ~6500 167 ~0.026 ~0.5 \\((1-0.013/0.026)\\) vaccine ~6500 85 ~0.013 Turkey placebo 570 26 ~0.046 ~0.913 \\((1-0.004/0.046)\\) vaccine 752 3 ~0.004 FYI: The vaccine efficacy is ‘is the percentage reduction of disease in a vaccinated group of people compared to an unvaccinated group’, we will calculate it as \\(1-incidence_{vaccine}/incidence_{placebo}\\) for the present purposes. I’d like to model it similarly to our dative model, as below. (Keep in mind, this is not a mechanistic model of infection, but a statistical model. A lens through which we choose to look at the data.) Figure 9.1: A schematic of the infection model. Our estimate of the vaccine efficacy would be \\(eff = 1-p_2/p_1\\). So how do we compute the likelihood of each data point given the parameters? In the dative data set, our data points were 0s and 1s. Each corresponded to a single observation. Here, our observations are aggregates (i.e., 26 out of 570). That’s a bit different. We could make up a data set with ‘single observations’ that matches this statistic, and it will work in many cases, but not always. There is a better solution, though. 9.0.2 Example 2 The mammalian sleep data set. How do we model the number of hours animals sleep? ## [1] 12 17 14 15 4 14 9 7 10 3 5 9 10 12 10 8 9 17 5 18 4 20 3 3 10 ## [26] 11 15 12 10 2 3 6 6 8 10 3 19 10 14 14 13 12 20 15 11 8 14 8 4 10 ## [51] 16 10 14 9 10 11 12 14 4 6 11 18 5 13 9 10 8 11 11 17 14 16 13 9 9 ## [76] 16 4 16 9 5 6 12 10 No idea. But we have to do it somehow, right? 9.1 Probability Distributions Luckily, our problems are not unique, and very cool solutions have been developed. For many problems, it has been worked out what the probability distributions of some common generative processes are. A probability distribution is an assignment of probabilites to events (i.e., an assignment of numbers such that it follows the laws of probability). A probability distribution with discrete outcomes can be characterized by its probability mass function (PMF). Example: I could describe the outcomes of an experiment where I take one marble from a bag with one blue and three marbles with either of the following ditributions, depending on whether I’m interested in the particular marbles identity, or just their color: Distribution 1: \\(P(marble~1~[blue] ) = 0.25\\) \\(P(marble~2~[white]) = 0.25\\) \\(P(marble~3~[white]) = 0.25\\) \\(P(marble~4~[white]) = 0.25\\) Distribution 2: \\(P(blue~marble~[1]) = 0.25~~~~~~~~~~~\\) \\(P(white~marble~[2,3,~or~4]) = 0.75\\) 9.1.1 Bernoulli Distribution Below is the probability mass function for a Bernoulli distribution with the parameter \\(p=.6\\). Generative process assumed by the Bernoulli distribution: a single “trial” which can result in either“success” or “failure”. Figure 9.2: A schematic of the infection model. Examples: coin flip (H/T), drawing one marble (W/B), shooting (H/M) library(ggplot2) df &lt;- data.frame(successes = 0:1, prob = dbinom(0:1, size = 1, prob = .6) ) ggplot(df, aes(successes, prob)) + geom_bar(stat = &quot;identity&quot;, width = 0.1)+scale_x_continuous(breaks=0:1) + facet_wrap(~&quot;Bernoulli Distribution (p=.6)&quot;) + theme_bw() 9.2 Binomial Distribution Generative process assumed by the binomial distribution: \\(n\\) trials with a fixed probability \\(p\\) of success. We count the number of successes. Figure 9.3: A schematic of the infection model. Below are the PMFs for a binomial distribution with the parameters \\(n={2,10}\\) and \\(p={0.3, 0.7}\\). The value at every point \\(k\\) represents the probability of \\(k\\) successes out of \\(n\\) attempts if the probability of a success on each trial is \\(p\\). Examples of such processes are: flipping coins, looking at shirts in a store, yes/no acceptability judgements, searching a corpus, lending money, … Numbers generated by a mechanism that fits this description will will be distributed binomially (\\(X \\sim B(n, k)\\)). There is an equation for the PMF, but to us it’s simply dbinom(..., n, p), pbinom() and rbinom(...). 9.2.1 Gaussian Distribution (‘Normal Distribution’) Generative process assumed by the gassian distribution: it is limiting case of a ‘substantial’ number of factors contributing to an outcome positively or negatively. Think about the generative process roughly as below: Figure 9.4: A schematic of the infection model. There is an equation for the PDF, but to us it’s simply dnorm(..., n, p), pnorm() and rnorm(...). 9.3 A likelihood function for the vaccine data country group total participants COVID-19 cases [log-]likelihood Brazil placebo ~6500 167 dbinom(167, 6500, \\(p_{1B}\\)[, log=T]) vaccine ~6500 85 dbinom(85, 6500, \\(p_{2B}\\)[, log=T]) Turkey placebo 570 26 dbinom(26, 570, \\(p_{1T}\\)[, log=T]) vaccine 752 3 dbinom(3, 752, \\(p_{2T}\\)[, log=T]) OK. So let’s write one log-likelihood function for the Brazil trial, and one for the Turkey trial and plot the log-likelihood as a function of the parameters \\(p_{1B,T}\\) and \\(p_{2B,T}\\). 9.4 Bayesian Inference I Now we know the relative likelihood of the data under different parameters \\(\\theta\\) (in our case: \\(p_1, p_2\\)), i.e., \\(P(D|\\theta)\\). [Think of \\(\\theta\\) as something of a pronoun for parameters.] But I want to know the relative likelihood of the parameters given the data \\(P(\\theta|D)\\). According to the definition of conditional probability, we know two things to be true: \\(P(H|D) = P(H, D) / P(D)\\) \\(\\Longrightarrow\\) \\(P(H|D) \\cdot P(D) = P(H, D)\\) \\(P(D|H) = P(D, H) / P(H)\\) \\(\\Longrightarrow\\) \\(P(D|H) \\cdot P(H) = P(D, H)\\) Because \\(P(H, D)\\) and \\(P(H, D)\\) are the same thing (the joint probability of \\(H\\) and \\(D\\)), the left sides of the above equations in brackets are also equal: \\(P(H|D) \\cdot P(D) = P(D|H) \\cdot P(H)\\) \\(\\Longrightarrow\\) \\(P(H|D) = \\frac{ P(D|H) \\cdot P(H) }{P(D)}\\) The result we just obtained is known as the Bayes Theorem: \\[P(H|D) = \\frac{ P(D|H) \\cdot P(H) }{P(D)}\\] P(H|D): probability of a hypothesis after seeing the data, or the posterior probability of the hypothesis. P(D|H): probability of seeing such data under that hypothesis, or the likelihood of the data under the hypothesis. P(H): probability of the hypothesis being true, or the prior probability of the hypothesis. P(D): probability of seeing such data under any hypothesis, or the marginal probability of the data. In other words: \\[Posterior~prob. = \\frac{ Likelihood \\cdot Prior~prob. }{Marginal~prob.~of~the~data}\\] 9.5 Returning to Example 1 Let’s use only the placebo arm of the Brazil trial, i.e., ‘167 events on ~6500 trials’ to estimate a plausible range for the incidence in similar groups, under similar conditions. We know that the parameter \\(p_{1B}\\) is probably somewhere in the vicinity of the incidence rate (i.e., \\(0.026\\)). The question is, how close a vicinity? We’ll use the Bayes Theorem: 9.5.1 Likelihood Let’s use only the placebo group of the Brazil data and assume that we have only five possibly hypotheses: \\(H_1\\):\\(~p_1=0.020\\) \\(H_2\\):\\(~p_1=0.025\\) \\(H_3\\):\\(~p_1=0.030\\) \\(H_4\\):\\(~p_1=0.035\\) \\(H_5\\):\\(~p_1=0.040\\) p1B &lt;- c(.02, .025, .03, .035, .04) Let’s compute \\(P(D|H)\\), the likelihood of the data under each hypothesis. We know how to do it, just use dbinom. (likelihood &lt;- dbinom(x=167, size=6500, prob=p1B)) ## [1] 2.249124e-04 2.934130e-02 3.547394e-03 3.281750e-06 8.184725e-11 9.5.2 Prior Now what do we do about the prior? I have no idea. So let’s use what is called a flat prior: All hypotheses seem equally likely a-priori. Since there are 5 hypotheses, we therefore assign each hypothesis a probability of \\(1/5\\). (prior_p1B &lt;- rep(1/5, 5)) ## [1] 0.2 0.2 0.2 0.2 0.2 9.5.3 Marginal probability of the data So what is this marginal probability of the data? That is, the average probability of the data, i.e., under all possible hypotheses (weighted by their prior probability). \\[ P(D) = \\sum_{i} P(D|H_i) \\cdot P(H_i) \\] - Please note that it’s the sum of the expressions in the numerator. If the concept seems very alien to you, it may be based on the misconception that it’s a ‘real probability’, i.e., something that exists in the real world. However, it is model-based construct. That is, under the kind of model we assume generated the data, what is the probability of such data? Another way to think about \\(P(D)\\) is that it’s simply a ‘normalizing constant’. Its purpose it to make sure that the probabilities add up to 1. (marginal_prob_data &lt;- sum(likelihood * prior_p1B)) ## [1] 0.006623378 9.5.4 Posterior probability Remember, \\[Posterior~probability_{(H_i)} = \\frac{ Likelihood_{(H_i)} \\cdot Prior~probability_{(H_i)} }{Marginal~prob.~of~the~data}\\] Let’s use that to compute the posterior probability of our hypotheses. posterior_p1B &lt;- (likelihood * prior_p1B) / marginal_prob_data 9.5.5 And now again, for p1B &lt;- seq(0.015, 0.040, .001) # hypotheses likelihood &lt;- dbinom(x=167, size=6500, prob=p1B) # likelihood prior_p1B &lt;- rep(1/length(p1B), length(p1B)) # posterior marginal_prob_data &lt;- sum(likelihood * prior_p1B) # marginal prob of data posterior_p1B &lt;- (likelihood * prior_p1B) / marginal_prob_data # posterior for p1B ggplot(data=NULL, aes(x=p1B, y=posterior_p1B)) + geom_point() + geom_line() + theme_bw() #+ scale_x_continuous(limits=c(0,.1)) 9.6 Bayesian Inference II Luckily, we don’t have to do this every time. We can perform Bayesian with the R packages brms/rstan. In contrast to what we did above, though, we don’t get back a plot, but a number of samples from the posterior distribution. Let’s take a look at how it works: # load brms (you&#39;ll need to install &#39;brms&#39;, &#39;rstan&#39;, and possibly Rtools, depending on the system) library(brms) # define data frame df &lt;- data.frame(n_yes=167, n=6500) # fit brms model m1 &lt;- brm(n_yes|trials(n) ~ 1, # model specification data = df, # data family = binomial(&quot;identity&quot;), # assume that the outcome is binomially distributed cores = 4, # use 4 cores file = &quot;./models/model1&quot; # save to file (WARNING: if file exists, model won&#39;t be fitted again even if it changed) ) summary(m1) ## Family: binomial ## Links: mu = identity ## Formula: n_yes | trials(n) ~ 1 ## Data: df (Number of observations: 1) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.03 0.00 0.02 0.03 1.00 1458 1719 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As you see, the model … … provides coefficient estimates in the Estimate column under Population-Level Effects (estimate = \\(0.03\\); rounded). … provides credible intervals (95%) in the l-95% CI, and u-95% CI columns under Population-Level Effects (\\(CrI = [0.02; 0.03]\\)). The most important result is the 95% credible interval (usually ‘CrI’, sometimes ‘CI’). Our model tells us that with 95% probability, the parameter is in this range. # extract samples from the model and take a look at them samples1 &lt;- posterior_samples(m1) head(samples1, 5) ## b_Intercept lp__ ## 1 0.02728792 -20.31070 ## 2 0.02728792 -20.31070 ## 3 0.02182273 -22.15158 ## 4 0.02572990 -19.99310 ## 5 0.02499430 -20.05733 # compute exact credible interval (cri95 &lt;- HDInterval::hdi(samples1$b_Intercept)) ## lower upper ## 0.02217407 0.02998686 ## attr(,&quot;credMass&quot;) ## [1] 0.95 # plot the samples histogram ... p &lt;- ggplot(samples1, aes(b_Intercept)) + geom_histogram(breaks=c(p1B-.001/2,0.040+.001) ) # ... anong with our old approximation ... p &lt;- p + geom_point(data=data.frame(p1B, posterior_p1B), aes(x=p1B, y=posterior_p1B*4000), color = &quot;red&quot;) + geom_line(data=data.frame(p1B, posterior_p1B), aes(x=p1B, y=posterior_p1B*4000), color = &quot;red&quot;) # ... and the 95% CrI p + geom_vline(xintercept = cri95[&#39;lower&#39;], color = &quot;blue&quot;) + geom_vline(xintercept = cri95[&#39;upper&#39;], color = &quot;blue&quot;) 9.7 Back to our incidence rates 9.7.1 Brazil Estimate a model under the assumption that \\(p = a + b \\cdot c\\_is\\_vaccine\\), where c_is_vaccine is a centered predictor. Intercept: \\(a\\) is the average incidence Slope: \\(b\\) is the difference between \\(p_{1B}\\) and \\(p_{1T}\\) Remember, the model will return samples that represent the posterior probability distribution over the parameters \\(a\\) and \\(b\\), not the samples! # define data frame df2 &lt;- data.frame(n_yes = c(167, 85), n = c(6500, 6500), c_is_vaccine = c(-0.5, 0.5) ) # fit brms model m2 &lt;- brm(n_yes|trials(n) ~ 1 + c_is_vaccine, # model specification data = df2, # data family = binomial(&quot;identity&quot;), # assume that the outcome is binomially distributed cores = 4, # use 4 cores file = &quot;./models/model2&quot; # save to file (WARNING: if file exists, model won&#39;t be fitted again even if it changed) ) summary(m2) ## Family: binomial ## Links: mu = identity ## Formula: n_yes | trials(n) ~ 1 + c_is_vaccine ## Data: df2 (Number of observations: 2) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.02 0.00 0.02 0.02 1.00 2462 2378 ## c_is_vaccine -0.01 0.00 -0.02 -0.01 1.00 1395 1502 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). samples2 &lt;- posterior_samples(m2) head(samples2, 5) ## b_Intercept b_c_is_vaccine lp__ ## 1 0.02008888 -0.009867631 -14.53036 ## 2 0.01943221 -0.012430289 -13.49045 ## 3 0.02058865 -0.014269747 -14.03007 ## 4 0.02052979 -0.015209252 -14.22080 ## 5 0.01968251 -0.011715166 -13.62410 # apply the reverse logic of the linear model to get posterior distributions for p1B and p2B from the infection model above samples_p1B &lt;- samples2$b_Intercept - 0.5* samples2$b_c_is_vaccine samples_p2B &lt;- samples2$b_Intercept + 0.5* samples2$b_c_is_vaccine # use these samples to compute the posterior distribution of efficacy samples_eff_Brazil &lt;- 1 - samples_p2B/samples_p1B # compute exact credible interval (cri95_eff_Brazil &lt;- HDInterval::hdi(samples_eff_Brazil)) ## lower upper ## 0.3494192 0.6147580 ## attr(,&quot;credMass&quot;) ## [1] 0.95 # plot the samples histogram ... p &lt;- ggplot(data=NULL, aes(samples_eff_Brazil)) + geom_histogram(bins=50) # ... and the 95% CrI p + geom_vline(xintercept = cri95_eff_Brazil[&#39;lower&#39;], color = &quot;blue&quot;) + geom_vline(xintercept = cri95_eff_Brazil[&#39;upper&#39;], color = &quot;blue&quot;) 9.7.2 Turkey Now let’s do the same for the Turkey trial … "],["case-studies.html", "Chapter 10 Case Studies 10.1 VOT Data 10.2 Priming Experiment", " Chapter 10 Case Studies 10.1 VOT Data Data on voicing in English and Korean. 10.1.1 Loading the Data # read English data and keep only relevant columns vot_engl &lt;- readxl::read_excel(&quot;./data/VOT/english.xlsx&quot;, sheet = 1) %&gt;% dplyr::select(subject, gender, Item, WorldBet, TargetConsonant, TargetVowel, msVOT) %&gt;% subset(TargetConsonant %in% c(&quot;d&quot;,&quot;g&quot;,&quot;th&quot;,&quot;kh&quot;)) ## New names: ## * `` -&gt; ...20 ## * `` -&gt; ...21 # take a look head(vot_engl) ## # A tibble: 6 x 7 ## subject gender Item WorldBet TargetConsonant TargetVowel msVOT ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 e9gt01mw m dnky4 dAn.ki d A 13.6250605772297 ## 2 e9gt01mw m door5 do9 d o 9.86782693455979 ## 3 e9gt01mw m tnge5 th^N th ^ 66.9408128957514 ## 4 e9gt01mw m cave3n khev kh e 60.3012519920512 ## 5 e9gt01mw m gmdp5 g^m.d9Aps g ^ 14.2080928800823 ## 6 e9gt01mw m domz3 domz d o 10.724777403027 # which consonants are we dealing with, anyway unique(vot_engl$TargetConsonant) ## [1] &quot;d&quot; &quot;th&quot; &quot;kh&quot; &quot;g&quot; # Worldbet documentation in the appendix: # http://www.cs.toronto.edu/~frank/csc401/readings/worldbet.pdf # read Korean data and keep only relevant columns vot_kor &lt;- readr::read_tsv(&quot;./data/VOT/songyuan.txt&quot;) %&gt;% dplyr::select(subject, gender, Item, WorldBet, TargetConsonant, TargetVowel, msVOT) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## utt = col_character(), ## subject = col_character(), ## gender = col_character(), ## StartTime = col_double(), ## EndTime = col_double(), ## Item = col_character(), ## WorldBet = col_character(), ## TargetConsonant = col_character(), ## TargetVowel = col_character(), ## Burst = col_double(), ## VOT = col_double(), ## VowelEnd = col_double(), ## Notes = col_character(), ## msVOT = col_double(), ## msVowel = col_double(), ## logVOT = col_double(), ## logVowel = col_double() ## ) # take a look head(vot_kor) ## # A tibble: 6 x 7 ## subject gender Item WorldBet TargetConsonant TargetVowel msVOT ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 s9gt01fw f tnka3 tang2.kuai4 t a 82.3 ## 2 s9gt01fw f duzi4 du4.zi d u 9.72 ## 3 s9gt01fw f dnzi3 ding1.zi d i 10.7 ## 4 s9gt01fw f tuzi5 tu4.zi t u 101. ## 5 s9gt01fw f kuzi5 ku4.zi k u 119. ## 6 s9gt01fw f guto3 gu3.tou g u 32.2 # which consonants are we dealing with, anyway unique(vot_kor$TargetConsonant) ## [1] &quot;t&quot; &quot;d&quot; &quot;k&quot; &quot;g&quot; typeof(vot_engl$msVOT) ## [1] &quot;character&quot; # let&#39;s see if all msVOT are convertible to doubles: subset(vot_engl, is.na(as.double(msVOT))) ## Warning in eval(e, x, parent.frame()): NAs introduced by coercion ## # A tibble: 4 x 7 ## subject gender Item WorldBet TargetConsonant TargetVowel msVOT ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 e9gt02mw m cuki4 khU.ki kh U but is not steep a… ## 2 e9gt02mw m tail4 the.&amp;l th e but less steep @ 8… ## 3 e9gt02mw m torn3 tho9n th o visually small in … ## 4 e9gt02mw m dign5 dI.gIxN d I high intensity and… # convert msVOT to a double vot_engl$msVOT %&lt;&gt;% as.double() ## Warning in vot_engl$msVOT %&lt;&gt;% as.double(): NAs introduced by coercion vot_kor$msVOT %&lt;&gt;% as.double() vot_engl$language &lt;- &quot;English&quot; vot_kor$language &lt;- &quot;Korean&quot; vot &lt;- dplyr::bind_rows(vot_engl, vot_kor) 10.1.2 Explorotary Data Analysis and Descriptive Statistics Let’s try to understand our data better. 10.1.2.1 Averages Let’s look a the average VOTs. vot_avg &lt;- vot %&gt;% group_by(language, TargetConsonant) %&gt;% dplyr::summarise(avg_VOT = mean(msVOT, na.rm=T)) vot_avg %&gt;% ggplot(aes(TargetConsonant, avg_VOT)) + geom_bar(stat = &quot;identity&quot;)+ facet_wrap(~language) 10.1.2.2 Histograms Because we have relatively few target sounds, and relatively many data points per sound, we can take a look at histograms by sound. ggplot(vot, aes(msVOT)) + geom_histogram() + facet_wrap(language~TargetConsonant, scales = &quot;free&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 4 rows containing non-finite values (stat_bin). 10.1.2.3 Boxplots Let’s look at them side-by-side, using ‘box-(and whiskers)-plots’. Boxplots provide a five-number summary of a distribution: The median, the 1st quartile (25th percentile), and the 3rd quartile (75th percentile) make up the box part. The upper whisker extends from the third quartile to the largest observed value no larger than \\(1.5\\cdot IQR\\) from it. The upper whisker extends from the first quartile to the smallerst observed value no smaller than \\(1.5\\cdot IQR\\) from it. ggplot(vot, aes(TargetConsonant, msVOT)) + geom_boxplot() + facet_wrap(~language, scales = &quot;free_x&quot;) ## Warning: Removed 4 rows containing non-finite values (stat_boxplot). 10.1.2.4 Summary Voiced stops clearly differ from voiceless stops in VOT. (What a surprise! :)) A lot of variation in the English dataset. Lower average VOTs in English. Picture is a bit more fuzzy for the median. 10.1.3 How long is the difference between voiced and unvoiced stops in English and in Korean? Now is the time to fit a linear model to estimate the differences between mean VOTs. Let’s first create the right contrasts. # let&#39;s set up the right contrasts unique(vot$TargetConsonant) ## [1] &quot;d&quot; &quot;th&quot; &quot;kh&quot; &quot;g&quot; &quot;t&quot; &quot;k&quot; df_contrasts &lt;- data.frame(TargetConsonant = c(&quot;d&quot;, &quot;g&quot;, &quot;th&quot;, &quot;kh&quot;, &quot;t&quot;, &quot;k&quot;), is_voiced = c(1, 1, 0, 0, 0, 0), is_velar = c(0, 1, 0, 1, 0, 1) ) # Create centered contrasts from dummy contrasts. That way, we&#39;ll be free to use whichever ones we like further down. df_contrasts$c_is_voiced &lt;- df_contrasts$is_voiced - 0.5 df_contrasts$c_is_velar &lt;- df_contrasts$is_velar - 0.5 # merge the contrasts into the current data frame # see cheat sheet for left_join: https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf vot %&lt;&gt;% dplyr::left_join(df_contrasts) ## Joining, by = &quot;TargetConsonant&quot; # code the language too vot$is_english &lt;- vot$language %&gt;% dplyr::recode(&quot;English&quot;=1, &quot;Korean&quot;=0) vot$c_is_english &lt;- vot$is_english Let’s start step-by-step and look at a range of models. In your term paper you will only need to present the one which is the most relevant. library(brms) vot_english &lt;- vot %&gt;% subset(language == &quot;English&quot;) # An intercept-only model. We use it to check that all packages are properly installed m1 &lt;- brm(msVOT ~ 1, family = gaussian(), data = vot_english %&gt;% subset(!is.na(msVOT)), file = &quot;./models/vot_m1&quot;, cores = 4) summary(m1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: msVOT ~ 1 ## Data: vot_english (Number of observations: 1011) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 41.19 1.68 37.97 44.49 1.00 3419 2829 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 53.46 1.20 51.22 55.90 1.00 3152 2667 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s look at model m1 in more detail. Here is what that brm call does: It says, let’s assume that all our observations are from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), and that \\(\\mu\\) = a (the intercept). Assuming that this model is correct, what can we say about the posterior distribution of the parameters \\(a\\) and \\(\\sigma\\) given the data? Well, it seems that we can say that with 95% probability \\(a\\) is in the interval \\([38; 44.5]\\), and \\(\\sigma\\) \\([51; 56]\\). # A model with voicing as a predictor. What do the coefs mean? m2a &lt;- brm(msVOT ~ 1 + is_voiced, family = gaussian(), data = vot_english %&gt;% subset(!is.na(msVOT)), file = &quot;./models/vot_m2a&quot;, cores = 4) summary(m2a) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: msVOT ~ 1 + is_voiced ## Data: vot_english %&gt;% subset(!is.na(msVOT)) (Number of observations: 1011) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 74.55 1.45 71.78 77.36 1.00 3910 2959 ## is_voiced -81.02 2.32 -85.63 -76.60 1.00 3904 2838 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.54 0.78 34.06 37.09 1.00 4024 2788 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # A model with voicing as a predictor. What do the coefs mean? m2b &lt;- brm(msVOT ~ 1 + c_is_voiced, family = gaussian(), data = vot_english %&gt;% subset(!is.na(msVOT)), file = &quot;./models/vot_m2b&quot;, cores = 4) summary(m2b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: msVOT ~ 1 + c_is_voiced ## Data: vot_english %&gt;% subset(!is.na(msVOT)) (Number of observations: 1011) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 34.04 1.17 31.74 36.30 1.00 3767 2618 ## c_is_voiced -81.09 2.26 -85.63 -76.48 1.00 3973 2397 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 35.52 0.77 34.06 37.05 1.00 3585 3008 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As you see, the slopes for voicing are the same, but the intercepts change if we change the predictor from dummy coding to a centered predictor. Let’s look a the language too: # A model with voicing and language as predictors. What do the coefs mean? m3 &lt;- brm(msVOT ~ 1 + c_is_english + c_is_voiced + c_is_english:c_is_voiced, family = gaussian(), data = vot %&gt;% subset(!is.na(msVOT)), file = &quot;./models/vot_m3&quot;, cores = 4) summary(m3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: msVOT ~ 1 + c_is_english + c_is_voiced + c_is_english:c_is_voiced ## Data: vot %&gt;% subset(!is.na(msVOT)) (Number of observations: 1410) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## Intercept 52.49 1.58 49.39 55.55 1.00 3577 ## c_is_english -18.48 1.90 -22.14 -14.72 1.00 3260 ## c_is_voiced -65.92 3.12 -71.98 -59.93 1.00 2371 ## c_is_english:c_is_voiced -15.14 3.73 -22.25 -7.81 1.00 2430 ## Tail_ESS ## Intercept 2738 ## c_is_english 2420 ## c_is_voiced 2645 ## c_is_english:c_is_voiced 2052 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 31.08 0.60 29.97 32.31 1.00 3546 2234 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The average effect of language on VOT (for both, voiced and voiceless consonants), is \\([-22; -15]\\). The average effect of voicing on VOT (across languages), is \\([-72; -60]\\). The interaction between language and voicing for VOT (the difference in the effect of voicing between languages), is \\([-22; -8]\\). Here is what happens if we remove all those odd VOT values. (They are all smaller than 0.) But can we even interpret the result if we selectively exclude data? # A model with voicing and language as predictors. What do the coefs mean? m3b &lt;- brm(msVOT ~ 1 + c_is_english + c_is_voiced + c_is_english:c_is_voiced, family = gaussian(), data = vot %&gt;% subset(!is.na(msVOT)) %&gt;% subset(msVOT &gt;= 0), cores = 4) ## Compiling Stan program... ## Start sampling summary(m3b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: msVOT ~ 1 + c_is_english + c_is_voiced + c_is_english:c_is_voiced ## Data: vot %&gt;% subset(!is.na(msVOT)) %&gt;% subset(msVOT &gt;= (Number of observations: 1333) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## Intercept 52.49 0.90 50.75 54.27 1.00 3807 ## c_is_english -7.81 1.07 -9.96 -5.71 1.00 3538 ## c_is_voiced -66.14 1.80 -69.68 -62.60 1.00 2379 ## c_is_english:c_is_voiced 6.36 2.19 2.06 10.70 1.00 2443 ## Tail_ESS ## Intercept 2544 ## c_is_english 2490 ## c_is_voiced 2454 ## c_is_english:c_is_voiced 2536 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 17.63 0.34 16.97 18.31 1.00 3897 2897 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Certainly, some values should be excluded. For example, \\(-150\\,ms\\) seems like an absurdly low VOT value. But all of them? How about \\(-30\\,ms\\). This is where domain knowledge is required (which I don’t have). 10.2 Priming Experiment # Load the &#39;primingHeid&#39; dataset library(languageR) priming_data &lt;- languageR::primingHeid %&gt;% dplyr::select(subject=Subject, word=Word, condition=Condition, RT=RT, response_correct=ResponseToPrime) priming_data$RT %&lt;&gt;% exp() priming_data$condition %&lt;&gt;% dplyr::recode(&quot;heid&quot;=&quot;related&quot;, &quot;baseheid&quot;=&quot;unrelated&quot;) head(priming_data) ## subject word condition RT response_correct ## 1 pp1 basaalheid related 807 correct ## 2 pp1 markantheid unrelated 906 correct ## 3 pp1 ontroerdheid unrelated 671 correct ## 4 pp1 contentheid related 718 correct ## 5 pp1 riantheid related 951 correct ## 6 pp1 tembaarheid unrelated 572 incorrect priming_data %&gt;% ggplot(aes(condition, RT)) + geom_boxplot() priming_data %&gt;% group_by(condition) %&gt;% dplyr::summarise(accuracy = mean(response_correct == &quot;correct&quot;)) ## # A tibble: 2 x 2 ## condition accuracy ## &lt;fct&gt; &lt;dbl&gt; ## 1 unrelated 0.925 ## 2 related 0.805 priming_correct &lt;- priming_data %&gt;% subset(response_correct == &quot;correct&quot;) priming_correct %&gt;% ggplot(aes(condition, RT)) + geom_boxplot() priming_correct %&gt;% group_by(condition) %&gt;% dplyr::summarise(mean_RT = mean(RT)) ## # A tibble: 2 x 2 ## condition mean_RT ## &lt;fct&gt; &lt;dbl&gt; ## 1 unrelated 735. ## 2 related 747. library(brms) priming_correct$c_is_related &lt;- priming_correct$condition %&gt;% dplyr::recode(&quot;unrelated&quot;=-.5, &quot;related&quot;=.5) priming_m1 &lt;- brm(RT ~ c_is_related, data = priming_correct, file = &quot;./models/priming_m1&quot;) summary(priming_m1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: RT ~ c_is_related ## Data: priming_correct (Number of observations: 719) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 741.05 8.84 723.39 758.75 1.00 3718 2777 ## c_is_related 12.21 17.93 -23.00 47.06 1.00 3595 2690 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 237.42 6.26 225.47 250.33 1.00 4642 3162 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). "],["your-term-paper.html", "Chapter 11 Your term paper", " Chapter 11 Your term paper Please download the data from moodle: ‘Priming experiment data’ (data_priming.csv) Columns: subject_id: The subject ID condition: One of condition_related, condition_unrelated, filler item: Item ID word: The target word RT: The response time in milliseconds response_yes: \\(1\\) if the response was ‘is a word’, and \\(0\\) is the response was ‘is not a word’ Approximately 3-6 pages Three sections: Method, Results, Conclusion Analysis code in a separate R file Method: What was the question we wanted to answer? (Imagine that you don’t know anything about previous priming studies.) Stimuli: What kind of words did we select? How were they selected? How was one condition different from the other? What were the fillers like? How many of which kind did participants see? Procedure: What did a single trial look like? Results: Descriptive statistics Inferential statics (brms model; contact me or Utku if you have trouble in setting up brms – we can run the model for you if all else fails) Conclusion? Do we have evidence for priming? (Could we have done anything better?) "]]
